{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import vaex\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report, confusion_matrix\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import sys\n",
    "import swifter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T08:20:37.375486Z",
     "start_time": "2023-06-13T08:20:36.788953Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hex_to_int(hex_value):\n",
    "    return int(hex_value, base=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T08:20:38.140472Z",
     "start_time": "2023-06-13T08:20:37.634203Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hex_string_to_array(hex_string):\n",
    "    if hex_string == 'z':\n",
    "        return []\n",
    "    else:\n",
    "        return list(map(hex_to_int, hex_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T08:20:39.212536Z",
     "start_time": "2023-06-13T08:20:38.305541Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 13, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex_string_to_array('0D0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T08:20:39.829934Z",
     "start_time": "2023-06-13T08:20:39.281680Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_flag(sample):\n",
    "    if not isinstance(sample['Flag'], str):\n",
    "        col = 'Data' + str(sample['DLC'])\n",
    "        sample['Flag'] = sample[col]\n",
    "        sample[col] = '00'\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T08:20:40.706719Z",
     "start_time": "2023-06-13T08:20:40.051731Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/car-hacking/Test_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Read by dask first\n",
    "attributes = ['Timestamp', 'canID', 'DLC',\n",
    "   'Data0', 'Data1', 'Data2',\n",
    "           'Data3', 'Data4', 'Data5',\n",
    "                           'Data6', 'Data7', 'Flag']\n",
    "dataset_path  = '../data/car-hacking/'\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM', 'Test']\n",
    "attack = attack_types[4]\n",
    "file_name = '{}{}_dataset.csv'.format(dataset_path, attack)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(pe, time_position):\n",
    "    # 根据时间位置切分出对应的位置编码\n",
    "    pe = torch.index_select(pe, 0, time_position)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Timestamp canID  DLC           Payload   Flag\n",
      "0   1.478196e+09  0545    8  d800008a00000000  False\n",
      "1   1.478196e+09  02b0    5  ff7f000549000000  False\n",
      "2   1.478196e+09  0002    8  0000000000010715  False\n",
      "3   1.478196e+09  0153    8  002110ff00ff0000  False\n",
      "4   1.478196e+09  0130    8  198000fffe7f0760  False\n",
      "..           ...   ...  ...               ...    ...\n",
      "95  1.478196e+09  0260    8  18212130088f7006  False\n",
      "96  1.478196e+09  02a0    8  0400991d9702bd00  False\n",
      "97  1.478196e+09  0329    8  40b87f1411200014  False\n",
      "98  1.478196e+09  0545    8  d855008b00000000  False\n",
      "99  1.478196e+09  02b0    5  ff7f00053e000000  False\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "HEADER BEFORE:  tensor([[ 0,  5,  4,  5],\n",
      "        [ 0,  2, 11,  0],\n",
      "        [ 0,  0,  0,  2],\n",
      "        [ 0,  1,  5,  3],\n",
      "        [ 0,  1,  3,  0],\n",
      "        [ 0,  1,  3,  1],\n",
      "        [ 0,  1,  4,  0],\n",
      "        [ 0,  3,  5,  0],\n",
      "        [ 0,  2, 12,  0],\n",
      "        [ 0,  3,  7,  0],\n",
      "        [ 0,  4,  3, 15],\n",
      "        [ 0,  4,  4,  0],\n",
      "        [ 0,  4, 15,  0],\n",
      "        [ 0,  3,  1,  6],\n",
      "        [ 0,  1,  8, 15],\n",
      "        [ 0,  2,  6,  0],\n",
      "        [ 0,  2, 10,  0],\n",
      "        [ 0,  3,  2,  9],\n",
      "        [ 0,  5,  4,  5],\n",
      "        [ 0,  2, 11,  0],\n",
      "        [ 0,  4,  3,  0],\n",
      "        [ 0,  4, 11,  1],\n",
      "        [ 0,  1, 15,  1],\n",
      "        [ 0,  1,  5,  3],\n",
      "        [ 0,  0,  0,  2],\n",
      "        [ 0,  0, 10,  0],\n",
      "        [ 0,  0, 10,  1],\n",
      "        [ 0,  3,  5,  0],\n",
      "        [ 0,  2, 12,  0],\n",
      "        [ 0,  1,  3,  0],\n",
      "        [ 0,  1,  3,  1],\n",
      "        [ 0,  1,  4,  0],\n",
      "        [ 0,  3,  7,  0],\n",
      "        [ 0,  4,  3, 15],\n",
      "        [ 0,  4,  4,  0],\n",
      "        [ 0,  6,  9,  0],\n",
      "        [ 0,  3,  1,  6],\n",
      "        [ 0,  1,  8, 15],\n",
      "        [ 0,  2,  6,  0],\n",
      "        [ 0,  2, 10,  0],\n",
      "        [ 0,  3,  2,  9],\n",
      "        [ 0,  5,  4,  5],\n",
      "        [ 0,  2, 11,  0],\n",
      "        [ 0,  0,  0,  2],\n",
      "        [ 0,  1,  5,  3],\n",
      "        [ 0,  1,  3,  0],\n",
      "        [ 0,  1,  3,  1],\n",
      "        [ 0,  1,  4,  0],\n",
      "        [ 0,  3,  5,  0],\n",
      "        [ 0,  2, 12,  0],\n",
      "        [ 0,  3,  7,  0],\n",
      "        [ 0,  4,  3, 15],\n",
      "        [ 0,  4,  4,  0],\n",
      "        [ 0,  4, 15,  0],\n",
      "        [ 0,  3,  1,  6],\n",
      "        [ 0,  1,  8, 15],\n",
      "        [ 0,  2,  6,  0],\n",
      "        [ 0,  2, 10,  0],\n",
      "        [ 0,  3,  2,  9],\n",
      "        [ 0,  5,  4,  5],\n",
      "        [ 0,  2, 11,  0],\n",
      "        [ 0,  4,  3,  0],\n",
      "        [ 0,  4, 11,  1],\n",
      "        [ 0,  1, 15,  1],\n",
      "        [ 0,  1,  5,  3],\n",
      "        [ 0,  0,  0,  2],\n",
      "        [ 0,  3,  5,  0],\n",
      "        [ 0,  3,  7,  0],\n",
      "        [ 0,  4,  3, 15],\n",
      "        [ 0,  2, 12,  0],\n",
      "        [ 0,  1,  3,  0],\n",
      "        [ 0,  1,  3,  1],\n",
      "        [ 0,  1,  4,  0],\n",
      "        [ 0,  4,  4,  0],\n",
      "        [ 0,  3,  1,  6],\n",
      "        [ 0,  1,  8, 15],\n",
      "        [ 0,  2,  6,  0],\n",
      "        [ 0,  2, 10,  0],\n",
      "        [ 0,  3,  2,  9],\n",
      "        [ 0,  5,  4,  5],\n",
      "        [ 0,  5, 15,  0],\n",
      "        [ 0,  2, 11,  0],\n",
      "        [ 0,  0,  0,  2],\n",
      "        [ 0,  1,  5,  3],\n",
      "        [ 0,  1,  3,  0],\n",
      "        [ 0,  1,  3,  1],\n",
      "        [ 0,  1,  4,  0],\n",
      "        [ 0,  3,  5,  0],\n",
      "        [ 0,  2, 12,  0],\n",
      "        [ 0,  3,  7,  0],\n",
      "        [ 0,  4,  3, 15],\n",
      "        [ 0,  4,  4,  0],\n",
      "        [ 0,  4, 15,  0],\n",
      "        [ 0,  3,  1,  6],\n",
      "        [ 0,  1,  8, 15],\n",
      "        [ 0,  2,  6,  0],\n",
      "        [ 0,  2, 10,  0],\n",
      "        [ 0,  3,  2,  9],\n",
      "        [ 0,  5,  4,  5],\n",
      "        [ 0,  2, 11,  0]])\n",
      "PAYLOAD BEFORE:  tensor([[13,  8,  0,  ...,  0,  0,  0],\n",
      "        [15, 15,  7,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  7,  1,  5],\n",
      "        ...,\n",
      "        [ 4,  0, 11,  ...,  0,  1,  4],\n",
      "        [13,  8,  5,  ...,  0,  0,  0],\n",
      "        [15, 15,  7,  ...,  0,  0,  0]])\n",
      "0\n",
      "HEADER FEATURE:  [[ 0  5  4  5]\n",
      " [ 0  2 11  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  1  5  3]\n",
      " [ 0  1  3  0]\n",
      " [ 0  1  3  1]\n",
      " [ 0  1  4  0]\n",
      " [ 0  3  5  0]\n",
      " [ 0  2 12  0]\n",
      " [ 0  3  7  0]\n",
      " [ 0  4  3 15]\n",
      " [ 0  4  4  0]\n",
      " [ 0  4 15  0]\n",
      " [ 0  3  1  6]\n",
      " [ 0  1  8 15]\n",
      " [ 0  2  6  0]\n",
      " [ 0  2 10  0]\n",
      " [ 0  3  2  9]\n",
      " [ 0  5  4  5]\n",
      " [ 0  2 11  0]\n",
      " [ 0  4  3  0]\n",
      " [ 0  4 11  1]\n",
      " [ 0  1 15  1]\n",
      " [ 0  1  5  3]\n",
      " [ 0  0  0  2]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0 10  1]\n",
      " [ 0  3  5  0]\n",
      " [ 0  2 12  0]\n",
      " [ 0  1  3  0]\n",
      " [ 0  1  3  1]\n",
      " [ 0  1  4  0]\n",
      " [ 0  3  7  0]\n",
      " [ 0  4  3 15]\n",
      " [ 0  4  4  0]\n",
      " [ 0  6  9  0]\n",
      " [ 0  3  1  6]\n",
      " [ 0  1  8 15]\n",
      " [ 0  2  6  0]\n",
      " [ 0  2 10  0]\n",
      " [ 0  3  2  9]\n",
      " [ 0  5  4  5]\n",
      " [ 0  2 11  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  1  5  3]\n",
      " [ 0  1  3  0]\n",
      " [ 0  1  3  1]\n",
      " [ 0  1  4  0]\n",
      " [ 0  3  5  0]\n",
      " [ 0  2 12  0]\n",
      " [ 0  3  7  0]\n",
      " [ 0  4  3 15]\n",
      " [ 0  4  4  0]\n",
      " [ 0  4 15  0]\n",
      " [ 0  3  1  6]\n",
      " [ 0  1  8 15]\n",
      " [ 0  2  6  0]\n",
      " [ 0  2 10  0]\n",
      " [ 0  3  2  9]\n",
      " [ 0  5  4  5]\n",
      " [ 0  2 11  0]\n",
      " [ 0  4  3  0]\n",
      " [ 0  4 11  1]\n",
      " [ 0  1 15  1]\n",
      " [ 0  1  5  3]\n",
      " [ 0  0  0  2]\n",
      " [ 0  3  5  0]\n",
      " [ 0  3  7  0]\n",
      " [ 0  4  3 15]\n",
      " [ 0  2 12  0]\n",
      " [ 0  1  3  0]\n",
      " [ 0  1  3  1]\n",
      " [ 0  1  4  0]\n",
      " [ 0  4  4  0]\n",
      " [ 0  3  1  6]\n",
      " [ 0  1  8 15]\n",
      " [ 0  2  6  0]\n",
      " [ 0  2 10  0]\n",
      " [ 0  3  2  9]\n",
      " [ 0  5  4  5]\n",
      " [ 0  5 15  0]\n",
      " [ 0  2 11  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  1  5  3]\n",
      " [ 0  1  3  0]\n",
      " [ 0  1  3  1]\n",
      " [ 0  1  4  0]\n",
      " [ 0  3  5  0]\n",
      " [ 0  2 12  0]\n",
      " [ 0  3  7  0]\n",
      " [ 0  4  3 15]\n",
      " [ 0  4  4  0]\n",
      " [ 0  4 15  0]\n",
      " [ 0  3  1  6]\n",
      " [ 0  1  8 15]\n",
      " [ 0  2  6  0]\n",
      " [ 0  2 10  0]\n",
      " [ 0  3  2  9]\n",
      " [ 0  5  4  5]\n",
      " [ 0  2 11  0]]  AND LENGTH:  100\n",
      "PAYLOAD FEATURE:  [[13  8  0 ...  0  0  0]\n",
      " [15 15  7 ...  0  0  0]\n",
      " [ 0  0  0 ...  7  1  5]\n",
      " ...\n",
      " [ 4  0 11 ...  0  1  4]\n",
      " [13  8  5 ...  0  0  0]\n",
      " [15 15  7 ...  0  0  0]]  AND LENGTH:  100\n",
      "MASK FEATURE:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]  AND LENGTH:  100\n",
      "TIME FEATURE:  tensor([[-0.2624,  0.9650,  0.1316,  ...,  0.9999,  0.0079,  1.0000],\n",
      "        [-0.2624,  0.9650,  0.1316,  ...,  0.9999,  0.0079,  1.0000],\n",
      "        [-0.2624,  0.9650,  0.1316,  ...,  0.9999,  0.0079,  1.0000],\n",
      "        ...,\n",
      "        [-0.2624,  0.9650,  0.1316,  ...,  0.9999,  0.0079,  1.0000],\n",
      "        [-0.2624,  0.9650,  0.1316,  ...,  0.9999,  0.0079,  1.0000],\n",
      "        [-0.2624,  0.9650,  0.1316,  ...,  0.9999,  0.0079,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "df = dd.read_csv(file_name, header=None, names=attributes , dtype={5: 'object',\n",
    "       9: 'object', 7: 'object', 6: 'object', 11: 'object'})\n",
    "df = df.apply(fill_flag, axis=1, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'int64', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'float64', 'Data7': 'object', 'Flag': 'object'})\n",
    "\n",
    "df[['Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7']] = df[['Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7']].fillna('00')\n",
    "\n",
    "df['Payload'] = df[['Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7']].apply(lambda x: ''.join(x.astype(str)), axis=1, meta=(None, 'object'))\n",
    "\n",
    "pd_df = df.compute()\n",
    "pd_df = pd_df[['Timestamp', 'canID', 'DLC', 'Payload', 'Flag']].sort_values('Timestamp',  ascending=True)\n",
    "pd_df['Flag'] = pd_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "print(pd_df)\n",
    "\n",
    "header = np.array(list(map(hex_string_to_array, list(pd_df['canID']))))[:100]\n",
    "header = torch.from_numpy(header)\n",
    "print(\"HEADER BEFORE: \", header)\n",
    "\n",
    "sl_sum = np.array(list(map(hex_string_to_array, list(pd_df['Payload']))))\n",
    "sl_sum = torch.from_numpy(sl_sum)\n",
    "print(\"PAYLOAD BEFORE: \",sl_sum)\n",
    "\n",
    "ori_seq_len = header.shape[0]\n",
    "pad_len = 100 - ori_seq_len\n",
    "print(pad_len)\n",
    "\n",
    "header = F.pad(header.T, (0, pad_len)).T.numpy()\n",
    "sl_sum = F.pad(sl_sum.T, (0, pad_len)).T.numpy()\n",
    "\n",
    "if pad_len == 0:\n",
    "       mask = np.array([False] * ori_seq_len)\n",
    "else:\n",
    "       mask = np.concatenate((np.array([False] * ori_seq_len), np.array([True] * pad_len)))\n",
    "       \n",
    "time_record = pd_df['Timestamp']\n",
    "len_time_record = len(time_record)\n",
    "\n",
    "for i in range(len_time_record):\n",
    "       value = round(math.log(round(time_record[i] / 1e-6) + 1, 2))\n",
    "       time_record[i] = value\n",
    "for j in range(100 - len_time_record):\n",
    "       time_record = np.append(time_record, time_record[len_time_record - 1])\n",
    "\n",
    "pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / 40)) for i in range(40)] for pos in range(10000)])\n",
    "pe[:, 0::2] = np.sin(pe[:, 0::2])  # 偶数列用sin\n",
    "pe[:, 1::2] = np.cos(pe[:, 1::2])  # 奇数列用cos\n",
    "\n",
    "time_feature = get_time(pe, torch.IntTensor(time_record))\n",
    "sample = {'header': header, 'sl_sum': sl_sum, 'mask': mask, 'time': time_feature}\n",
    "\n",
    "print(\"HEADER FEATURE: \", header, \" AND LENGTH: \", len(header))\n",
    "print(\"PAYLOAD FEATURE: \", sl_sum, \" AND LENGTH: \", len(sl_sum))\n",
    "print(\"MASK FEATURE: \", mask, \" AND LENGTH: \", len(mask))\n",
    "print(\"TIME FEATURE: \", time_feature)\n",
    "# print(\"LABEL: \", label)\n",
    "# print(\"INDEX: \", idx)\n",
    "\n",
    "\n",
    "#print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T06:58:17.154779Z",
     "start_time": "2023-06-13T06:58:17.143542Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def draw_confusion(label_y, pre_y, path):\n",
    "    confusion = confusion_matrix(label_y, pre_y)\n",
    "    print(confusion)\n",
    "\n",
    "\n",
    "def write_result(fin, label_y, pre_y, classes_num):\n",
    "    if classes_num > 2:\n",
    "        accuracy = accuracy_score(label_y, pre_y)\n",
    "        macro_precision = precision_score(label_y, pre_y, average='macro')\n",
    "        macro_recall = recall_score(label_y, pre_y, average='macro')\n",
    "        macro_f1 = f1_score(label_y, pre_y, average='macro')\n",
    "        micro_precision = precision_score(label_y, pre_y, average='micro')\n",
    "        micro_recall = recall_score(label_y, pre_y, average='micro')\n",
    "        micro_f1 = f1_score(label_y, pre_y, average='micro')\n",
    "        print('  -- test result: ')\n",
    "        fin.write('  -- test result: \\n')\n",
    "        print('    -- accuracy: ', accuracy)\n",
    "        fin.write('    -- accuracy: ' + str(accuracy) + '\\n')\n",
    "        print('    -- macro precision: ', macro_precision)\n",
    "        fin.write('    -- macro precision: ' + str(macro_precision) + '\\n')\n",
    "        print('    -- macro recall: ', macro_recall)\n",
    "        fin.write('    -- macro recall: ' + str(macro_recall) + '\\n')\n",
    "        print('    -- macro f1 score: ', macro_f1)\n",
    "        fin.write('    -- macro f1 score: ' + str(macro_f1) + '\\n')\n",
    "        print('    -- micro precision: ', micro_precision)\n",
    "        fin.write('    -- micro precision: ' + str(micro_precision) + '\\n')\n",
    "        print('    -- micro recall: ', micro_recall)\n",
    "        fin.write('    -- micro recall: ' + str(micro_recall) + '\\n')\n",
    "        print('    -- micro f1 score: ', micro_f1)\n",
    "        fin.write('    -- micro f1 score: ' + str(micro_f1) + '\\n\\n')\n",
    "        report = classification_report(label_y, pre_y)\n",
    "        fin.write(report)\n",
    "        fin.write('\\n\\n')\n",
    "    else:\n",
    "        accuracy = accuracy_score(label_y, pre_y)\n",
    "        precision = precision_score(label_y, pre_y)\n",
    "        recall = recall_score(label_y, pre_y)\n",
    "        f1 = f1_score(label_y, pre_y)\n",
    "        print('  -- test result: ')\n",
    "        print('    -- accuracy: ', accuracy)\n",
    "        fin.write('    -- accuracy: ' + str(accuracy) + '\\n')\n",
    "        print('    -- recall: ', recall)\n",
    "        fin.write('    -- recall: ' + str(recall) + '\\n')\n",
    "        print('    -- precision: ', precision)\n",
    "        fin.write('    -- precision: ' + str(precision) + '\\n')\n",
    "        print('    -- f1 score: ', f1)\n",
    "        fin.write('    -- f1 score: ' + str(f1) + '\\n\\n')\n",
    "        report = classification_report(label_y, pre_y)\n",
    "        fin.write(report)\n",
    "        fin.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, d_in, d_out):  # config.slsum_count, config.dnn_out_d\n",
    "        super(DNN, self).__init__()\n",
    "        self.l1 = nn.Linear(d_in, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('x: ', x.numpy()[0])\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = F.relu(self.l3(out))\n",
    "        # print('dnn out: ', out.detach().numpy()[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, max_time_position, device):\n",
    "        super(Time_Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, time_position):\n",
    "        out = x.permute(1, 0, 2)\n",
    "        out = out + nn.Parameter(time_position, requires_grad=False).to(self.device)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrans(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MyTrans, self).__init__()\n",
    "        self.dnn = DNN(config.slsum_count, config.dnn_out_d)\n",
    "        self.head_dnn = DNN(60, config.head_dnn_out_d)\n",
    "        self.position_embedding = Time_Positional_Encoding(config.d_model, config.max_time_position, config.device).to(\n",
    "            config.device)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=config.d_model, nhead=config.nhead).to(config.device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=config.num_layers).to(\n",
    "            config.device)\n",
    "        self.fc = nn.Linear(config.d_model, config.classes_num).to(config.device)\n",
    "        self.pad_size = config.pad_size\n",
    "        self.dnn_out_d = config.dnn_out_d\n",
    "        self.head_dnn_out_d = config.head_dnn_out_d\n",
    "\n",
    "    def forward(self, header, sl_sum, mask, time_position):\n",
    "        dnn_out = torch.empty((sl_sum.shape[0], self.dnn_out_d, 0))\n",
    "\n",
    "        for i in range(self.pad_size):\n",
    "            tmp = self.dnn(sl_sum[:, i, :]).unsqueeze(2)\n",
    "            dnn_out = torch.concat((dnn_out, tmp), dim=2)\n",
    "        dnn_out = dnn_out.permute(0, 2, 1)\n",
    "\n",
    "        head_dnn_out = torch.empty((header.shape[0], self.head_dnn_out_d, 0))\n",
    "        for i in range(self.pad_size):\n",
    "            tmp = self.head_dnn(header[:, i, :]).unsqueeze(2)\n",
    "            head_dnn_out = torch.concat((head_dnn_out, tmp), dim=2)\n",
    "        head_dnn_out = head_dnn_out.permute(0, 2, 1)\n",
    "\n",
    "        x = torch.concat((head_dnn_out, dnn_out), dim=2).permute(1, 0, 2)\n",
    "\n",
    "        out = self.position_embedding(x, time_position)\n",
    "        out = self.transformer_encoder(out, src_key_padding_mask=mask)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = torch.sum(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Transformer'\n",
    "        self.slide_window = 2\n",
    "        self.slsum_count = int(math.pow(4, self.slide_window))  # 滑动窗口计数的特征的长度 n-gram?\n",
    "        self.dnn_out_d = 8  # 经过DNN后的滑动窗口计数特征的维度 Dimensions of sliding window count features after DNN\n",
    "        self.head_dnn_out_d = 2\n",
    "        self.d_model = self.dnn_out_d + self.head_dnn_out_d  # transformer的输入的特征的维度, dnn_out_d + 包头长度 The dimension of the input feature of the transformer, dnn_out_d + header length\n",
    "        self.pad_size = 100\n",
    "        self.max_time_position = 10000\n",
    "        self.nhead = 5\n",
    "        self.num_layers = 3\n",
    "        self.gran = 1e-6\n",
    "        self.log_e = 2\n",
    "        self.device = torch.device('cuda' if torch.backends.mps.is_available() else 'cpu')\n",
    "        self.classes_num = 2\n",
    "        self.batch_size = 10\n",
    "        self.epoch_num = 5\n",
    "        self.lr = 0.001\n",
    "        self.train_pro = 0.8  # 训练集比例 Ratio of training set\n",
    "\n",
    "        self.data_root_dir = '../data/car-hacking'\n",
    "        self.sl_sum_dir = '../data/car_hacking-data_slide_count_' + str(\n",
    "            self.slide_window) + '_arr'\n",
    "        self.time_dir = '../data/car_hacking-data_time'\n",
    "        self.names_file = '../data/name_class_CICIDS_3.csv'\n",
    "        self.model_save_path = '../model/' + self.model_name + '/'\n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.mkdir(self.model_save_path)\n",
    "        self.result_file = '/Users/d41sy/Desktop/sch/coding/ml-ids/result/trans8_performance.txt'\n",
    "\n",
    "        self.isload_model = False  # 是否加载模型继续训练 Whether to load the model and continue training\n",
    "        self.start_epoch = 24  # 加载的模型的epoch The epoch of the loaded model\n",
    "        self.model_path = 'model/' + self.model_name + '/' + self.model_name + '_model_' + str(self.start_epoch) + '.pth'  # 要使用的模型的路径 path to the model to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "fin = open(config.result_file, 'a')\n",
    "fin.write('-------------------------------------\\n')\n",
    "fin.write(config.model_name + '\\n')\n",
    "fin.write('begin time: ' + str(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())) + '\\n')\n",
    "fin.write('data root dir: ' + config.data_root_dir + '\\n')\n",
    "fin.write('sl_sum_dir: ' + config.sl_sum_dir + '\\n')\n",
    "fin.write('names_file: ' + config.names_file + '\\n')\n",
    "fin.write('d_model: ' + str(config.d_model) + '\\t pad_size: ' + str(config.pad_size) + '\\t nhead: ' + str(config.nhead)\n",
    "          + '\\t num_layers: ' + str(config.num_layers) + '\\t head_dnn_out_d: '+ str(config.head_dnn_out_d) +'\\n')\n",
    "fin.write(\n",
    "    'batch_size: ' + str(config.batch_size) + '\\t train pro: ' + str(config.train_pro) + '\\t learning rate: ' + str(\n",
    "        config.lr) + '\\n\\n')\n",
    "fin.close()\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetPreprocess(Dataset):\n",
    "    def __init__(self, root_path, payload_path, timestamp_path, names_file, pad_size, embed, max_time_position, gran, log_e, transform=None):\n",
    "        self.root_path = root_path\n",
    "        self.payload_path = payload_path\n",
    "        self.timestamp_path = timestamp_path\n",
    "        self.names_file = names_file # file contain list filenames of data\n",
    "        self.transform = transform\n",
    "        self.size = 0\n",
    "        self.name_list = []\n",
    "        self.pad_size = pad_size\n",
    "        self.embed = embed\n",
    "        self.max_time_position = max_time_position\n",
    "        self.gran = gran\n",
    "        self.log_e = log_e\n",
    "        \n",
    "        \n",
    "        if not os.path.isfile(self.names_file):\n",
    "            print(self.names_file + 'does not exist!')\n",
    "        f = open(self.names_file, 'r')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            self.name_list.append(line)\n",
    "            self.size += 1 # size of data files\n",
    "\n",
    "        self.pe = torch.tensor(\n",
    "            [[pos / (10000.0 ** (i // 2 * 2.0 / self.embed)) for i in range(self.embed)] for pos in\n",
    "             range(self.max_time_position)])\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])  # Use sin for even columns\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])  # Use cos for odd columns\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def get_time(self, time_position):\n",
    "        # Segment the corresponding position code according to the time position\n",
    "        pe = torch.index_select(self.pe, 0, time_position)\n",
    "        return pe\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"NAMELIST: \", self.name_list)\n",
    "        \n",
    "        # READ FILE CAR-HACKING | file_name {Fuzzy, DDos, RPM, GEAR}\n",
    "        df = dd.read_csv(file_name, header=None, names=attributes , dtype={5: 'object', 9: 'object', 7: 'object', 6: 'object', 11: 'object'})\n",
    "        df = df.apply(fill_flag, axis=1, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'int64', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'float64', 'Data7': 'object', 'Flag': 'object'})\n",
    "\n",
    "        # EXTRACTION\n",
    "        df[['Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7']] = df[['Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7']].fillna('00')\n",
    "        df['Payload'] = df[['Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7']].apply(lambda x: ''.join(x.astype(str)), axis=1, meta=(None, 'object'))\n",
    "        pd_df = df.compute()\n",
    "        pd_df = pd_df[['Timestamp', 'canID', 'DLC', 'Payload', 'Flag']].sort_values('Timestamp',  ascending=True)\n",
    "        pd_df['Flag'] = pd_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "        false_data = pd_df[pd_df['Flag'] == False]\n",
    "        # pd_df.head(30)\n",
    "\n",
    "        # PREPROCESS\n",
    "        ## GET HEADER \n",
    "        header = np.array(list(map(hex_string_to_array, list(false_data['canID']))))[:100]\n",
    "        header = torch.from_numpy(header)\n",
    "        # print(\"HEADER BEFORE: \", header)\n",
    "        ## GET PAYLOAD \n",
    "        payload = np.array(list(map(hex_string_to_array, list(false_data['Payload']))))\n",
    "        payload = torch.from_numpy(payload)\n",
    "        # print(\"PAYLOAD BEFORE: \",payload)\n",
    "\n",
    "        ori_seq_len = header.shape[0]\n",
    "        pad_len = 100 - ori_seq_len\n",
    "        # print(pad_len)\n",
    "\n",
    "        ## PAD WITH MAX SIZE = 100\n",
    "        header = F.pad(header.T, (0, pad_len)).T.numpy()\n",
    "        payload = F.pad(payload.T, (0, pad_len)).T.numpy()\n",
    "\n",
    "        if pad_len == 0:\n",
    "            mask = np.array([False] * ori_seq_len)\n",
    "        else:\n",
    "            mask = np.concatenate((np.array([False] * ori_seq_len), np.array([True] * pad_len)))\n",
    "        \n",
    "        ## GET TIMESTAMP \n",
    "        time_record = false_data['Timestamp']\n",
    "        len_time_record = len(time_record)\n",
    "\n",
    "        for i in range(len_time_record):\n",
    "            value = round(math.log(round(time_record[i] / self.gran) + 1, self.log_e))\n",
    "            time_record[i] = value\n",
    "        for j in range(self.pad_size - len_time_record):\n",
    "            time_record = np.append(time_record, time_record[len_time_record - 1])\n",
    "\n",
    "        time_feature = self.get_time(torch.IntTensor(time_record))\n",
    "        sample = {'header': header, 'sl_sum': payload, 'mask': mask, 'time': time_feature, 'label': 1, 'idx': idx}\n",
    "\n",
    "        # print(\"HEADER FEATURE: \", header, \" AND LENGTH: \", len(header))\n",
    "        print(\"PAYLOAD FEATURE: \", payload, \" AND LENGTH: \", len(payload[0]))\n",
    "        # print(\"MASK FEATURE: \", mask, \" AND LENGTH: \", len(mask))\n",
    "        # print(\"TIME FEATURE: \", time_feature)\n",
    "        # print(\"LABEL: \", label)\n",
    "        # print(\"INDEX: \", idx)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # print(\"SAMPLE: \", sample)\n",
    "        return sample\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetPreprocess(config.data_root_dir, config.sl_sum_dir, config.time_dir, config.names_file, config.pad_size, config.d_model, config.max_time_position, config.gran, config.log_e)\n",
    "size = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  16  TEST SIZE:  5  SIZE:  21\n",
      "finish load data\n"
     ]
    }
   ],
   "source": [
    "train_size = int(config.train_pro * size)\n",
    "test_size = size - train_size\n",
    "print(\"TRAIN SIZE: \", train_size, \" TEST SIZE: \", test_size, \" SIZE: \", size)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=config.batch_size)\n",
    "print('finish load data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case trained\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCase trained\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     model \u001b[39m=\u001b[39m MyTrans(config)\n\u001b[1;32m     11\u001b[0m     start_epoch \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     12\u001b[0m loss_func \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[162], line 8\u001b[0m, in \u001b[0;36mMyTrans.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dnn \u001b[39m=\u001b[39m DNN(\u001b[39m60\u001b[39m, config\u001b[39m.\u001b[39mhead_dnn_out_d)\n\u001b[1;32m      6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding \u001b[39m=\u001b[39m Time_Positional_Encoding(config\u001b[39m.\u001b[39md_model, config\u001b[39m.\u001b[39mmax_time_position, config\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mto(\n\u001b[1;32m      7\u001b[0m     config\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m----> 8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mTransformerEncoderLayer(d_model\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49md_model, nhead\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnhead)\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTransformerEncoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer, num_layers\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mnum_layers)\u001b[39m.\u001b[39mto(\n\u001b[1;32m     10\u001b[0m     config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39md_model, config\u001b[39m.\u001b[39mclasses_num)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/ptorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1149\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1146\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1147\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/ptorch/lib/python3.9/site-packages/torch/nn/modules/module.py:801\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    800\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 801\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    803\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    804\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    806\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    812\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ptorch/lib/python3.9/site-packages/torch/nn/modules/module.py:801\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    800\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 801\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    803\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    804\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    806\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    812\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ptorch/lib/python3.9/site-packages/torch/nn/modules/module.py:824\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 824\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    825\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    826\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/ptorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1147\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1145\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1146\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1147\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/miniconda3/envs/ptorch/lib/python3.9/site-packages/torch/cuda/__init__.py:258\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    261\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "if config.isload_model:\n",
    "    print(\"Case loaded\")\n",
    "    fin = open(config.result_file, 'a')\n",
    "    fin.write('load trained model :    model_path: ' + config.model_path)\n",
    "    model = torch.load(config.model_path)\n",
    "    start_epoch = config.start_epoch\n",
    "    fin.close()\n",
    "else:\n",
    "    print(\"Case trained\")\n",
    "    model = MyTrans(config)\n",
    "    start_epoch = -1\n",
    "loss_func = nn.CrossEntropyLoss().to(config.device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "for epoch in range(start_epoch + 1, config.epoch_num):\n",
    "    fin = open(config.result_file, 'a')\n",
    "    print('--- epoch ', epoch)\n",
    "    fin.write('-- epoch ' + str(epoch) + '\\n')\n",
    "    for i, sample_batch in enumerate(train_loader):\n",
    "        batch_header = sample_batch['header'].type(torch.FloatTensor).to(config.device)\n",
    "        batch_sl_sum = sample_batch['sl_sum'].type(torch.FloatTensor).to(config.device)\n",
    "        batch_mask = sample_batch['mask'].to(config.device)\n",
    "        batch_label = sample_batch['label'].to(config.device)\n",
    "        batch_time_position = sample_batch['time'].to(config.device)\n",
    "        print(\"BATCH HEADER: \", len(batch_header[0]), \" \\nBATCH PAYLOAD: \", batch_sl_sum[0][0],\" \\nBATCH MASK: \", len(batch_mask),\" \\nBATCH LABEL: \", batch_label,\" \\nBATCH TIME POSITION: \", batch_time_position[0])\n",
    "        out = model(batch_header, batch_sl_sum, batch_mask, batch_time_position)\n",
    "        loss = loss_func(out, batch_label)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % 20 == 0:\n",
    "            print('iter {} loss: '.format(i), loss.item())\n",
    "    torch.save(model, (config.model_save_path + config.model_name + '_model_{}.pth').format(epoch))\n",
    "\n",
    "    # test\n",
    "    label_y = []\n",
    "    pre_y = []\n",
    "    with torch.no_grad():\n",
    "        for j, test_sample_batch in enumerate(test_loader):\n",
    "            test_header = test_sample_batch['header'].type(torch.FloatTensor).to(config.device)\n",
    "            test_sl_sum = test_sample_batch['sl_sum'].type(torch.FloatTensor).to(config.device)\n",
    "            test_mask = test_sample_batch['mask'].to(config.device)\n",
    "            test_label = test_sample_batch['label'].to(config.device)\n",
    "            test_time_position = test_sample_batch['time'].to(config.device)\n",
    "            \n",
    "            test_out = model(test_header, test_sl_sum, test_mask, test_time_position)\n",
    "\n",
    "            pre = torch.max(test_out, 1)[1].cpu().numpy()\n",
    "            pre_y = np.concatenate([pre_y, pre], 0)\n",
    "            label_y = np.concatenate([label_y, test_label.cpu().numpy()], 0)\n",
    "        write_result(fin, label_y, pre_y, config.classes_num)\n",
    "    fin.close()\n",
    "\n",
    "fin = open(config.result_file, 'a')\n",
    "fin.write('\\n\\n\\n')\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def serialize_example(x, y):\n",
    "    \"\"\"converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    # Need to pay attention to whether it needs to be converted to numpy() form\n",
    "    input_features = tf.train.Int64List(value=np.array(x).flatten())\n",
    "    label = tf.train.Int64List(value=np.array([y]))\n",
    "    features = tf.train.Features(\n",
    "        feature={\n",
    "            \"input_features\": tf.train.Feature(int64_list=input_features),\n",
    "            \"label\": tf.train.Feature(int64_list=label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features=features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def write_tfrecord(data, filename):\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        tfrecord_writer.write(serialize_example(row['features'], row['label']))\n",
    "    tfrecord_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../data/car-hacking/Test_dataset.csv: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a82695b378e46ec8f87592aa76ff6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing: Done\n",
      "BYTEs:  8\n",
      "Data output [1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09\n",
      " 1.47819572e+09 1.47819572e+09 1.47819572e+09 1.47819572e+09]\n",
      "Data output [list([0, 5, 4, 5]) list([0, 5, 4, 5]) list([0, 5, 4, 5])\n",
      " list([0, 5, 4, 5]) list([0, 5, 4, 5]) list([0, 5, 4, 5])\n",
      " list([0, 2, 11, 0]) list([0, 2, 11, 0]) list([0, 2, 11, 0])\n",
      " list([0, 2, 11, 0]) list([0, 2, 11, 0]) list([0, 2, 11, 0])\n",
      " list([0, 0, 0, 2]) list([0, 0, 0, 2]) list([0, 0, 0, 2])\n",
      " list([0, 0, 0, 2]) list([0, 0, 0, 2]) list([0, 0, 0, 2])\n",
      " list([0, 1, 5, 3]) list([0, 1, 5, 3]) list([0, 1, 5, 3])\n",
      " list([0, 1, 5, 3]) list([0, 1, 5, 3]) list([0, 1, 5, 3])\n",
      " list([0, 1, 3, 0]) list([0, 1, 3, 0]) list([0, 1, 3, 0])\n",
      " list([0, 1, 3, 0]) list([0, 1, 3, 0]) list([0, 1, 3, 0])\n",
      " list([0, 1, 3, 1]) list([0, 1, 3, 1]) list([0, 1, 3, 1])\n",
      " list([0, 1, 3, 1]) list([0, 1, 3, 1]) list([0, 1, 3, 1])\n",
      " list([0, 1, 4, 0]) list([0, 1, 4, 0]) list([0, 1, 4, 0])\n",
      " list([0, 1, 4, 0]) list([0, 1, 4, 0]) list([0, 1, 4, 0])\n",
      " list([0, 3, 5, 0]) list([0, 3, 5, 0]) list([0, 3, 5, 0])\n",
      " list([0, 3, 5, 0]) list([0, 3, 5, 0]) list([0, 3, 5, 0])\n",
      " list([0, 2, 12, 0]) list([0, 2, 12, 0]) list([0, 2, 12, 0])\n",
      " list([0, 2, 12, 0]) list([0, 2, 12, 0]) list([0, 2, 12, 0])\n",
      " list([0, 3, 7, 0]) list([0, 3, 7, 0]) list([0, 3, 7, 0])\n",
      " list([0, 3, 7, 0]) list([0, 3, 7, 0]) list([0, 3, 7, 0])\n",
      " list([0, 4, 3, 15]) list([0, 4, 3, 15]) list([0, 4, 3, 15])\n",
      " list([0, 4, 3, 15]) list([0, 4, 3, 15]) list([0, 4, 3, 15])\n",
      " list([0, 4, 4, 0]) list([0, 4, 4, 0]) list([0, 4, 4, 0])\n",
      " list([0, 4, 4, 0]) list([0, 4, 4, 0]) list([0, 4, 4, 0])\n",
      " list([0, 4, 15, 0]) list([0, 4, 15, 0]) list([0, 4, 15, 0])\n",
      " list([0, 4, 15, 0]) list([0, 4, 15, 0]) list([0, 4, 15, 0])\n",
      " list([0, 3, 1, 6]) list([0, 3, 1, 6]) list([0, 3, 1, 6])\n",
      " list([0, 3, 1, 6]) list([0, 3, 1, 6]) list([0, 3, 1, 6])\n",
      " list([0, 1, 8, 15]) list([0, 1, 8, 15]) list([0, 1, 8, 15])\n",
      " list([0, 1, 8, 15]) list([0, 1, 8, 15]) list([0, 1, 8, 15])\n",
      " list([0, 2, 6, 0]) list([0, 2, 6, 0]) list([0, 2, 6, 0])\n",
      " list([0, 2, 6, 0]) list([0, 2, 6, 0]) list([0, 2, 6, 0])\n",
      " list([0, 2, 10, 0]) list([0, 2, 10, 0]) list([0, 2, 10, 0])\n",
      " list([0, 2, 10, 0])]\n",
      "Data output [list([216, 0, 0, 138, 0, 0, 0, 0]) list([216, 0, 0, 138, 0, 0, 0, 0])\n",
      " list([216, 0, 0, 138, 0, 0, 0, 0]) list([216, 0, 0, 138, 0, 0, 0, 0])\n",
      " list([216, 0, 0, 138, 0, 0, 0, 0]) list([216, 0, 0, 138, 0, 0, 0, 0])\n",
      " list([255, 127, 0, 5, 73, 0, 0, 0]) list([255, 127, 0, 5, 73, 0, 0, 0])\n",
      " list([255, 127, 0, 5, 73, 0, 0, 0]) list([255, 127, 0, 5, 73, 0, 0, 0])\n",
      " list([255, 127, 0, 5, 73, 0, 0, 0]) list([255, 127, 0, 5, 73, 0, 0, 0])\n",
      " list([0, 0, 0, 0, 0, 1, 7, 21]) list([0, 0, 0, 0, 0, 1, 7, 21])\n",
      " list([0, 0, 0, 0, 0, 1, 7, 21]) list([0, 0, 0, 0, 0, 1, 7, 21])\n",
      " list([0, 0, 0, 0, 0, 1, 7, 21]) list([0, 0, 0, 0, 0, 1, 7, 21])\n",
      " list([0, 33, 16, 255, 0, 255, 0, 0]) list([0, 33, 16, 255, 0, 255, 0, 0])\n",
      " list([0, 33, 16, 255, 0, 255, 0, 0]) list([0, 33, 16, 255, 0, 255, 0, 0])\n",
      " list([0, 33, 16, 255, 0, 255, 0, 0]) list([0, 33, 16, 255, 0, 255, 0, 0])\n",
      " list([25, 128, 0, 255, 254, 127, 7, 96])\n",
      " list([25, 128, 0, 255, 254, 127, 7, 96])\n",
      " list([25, 128, 0, 255, 254, 127, 7, 96])\n",
      " list([25, 128, 0, 255, 254, 127, 7, 96])\n",
      " list([25, 128, 0, 255, 254, 127, 7, 96])\n",
      " list([25, 128, 0, 255, 254, 127, 7, 96])\n",
      " list([23, 128, 0, 0, 101, 127, 7, 159])\n",
      " list([23, 128, 0, 0, 101, 127, 7, 159])\n",
      " list([23, 128, 0, 0, 101, 127, 7, 159])\n",
      " list([23, 128, 0, 0, 101, 127, 7, 159])\n",
      " list([23, 128, 0, 0, 101, 127, 7, 159])\n",
      " list([23, 128, 0, 0, 101, 127, 7, 159])\n",
      " list([0, 0, 0, 0, 2, 32, 39, 168]) list([0, 0, 0, 0, 2, 32, 39, 168])\n",
      " list([0, 0, 0, 0, 2, 32, 39, 168]) list([0, 0, 0, 0, 2, 32, 39, 168])\n",
      " list([0, 0, 0, 0, 2, 32, 39, 168]) list([0, 0, 0, 0, 2, 32, 39, 168])\n",
      " list([5, 32, 20, 104, 120, 0, 0, 33])\n",
      " list([5, 32, 20, 104, 120, 0, 0, 33])\n",
      " list([5, 32, 20, 104, 120, 0, 0, 33])\n",
      " list([5, 32, 20, 104, 120, 0, 0, 33])\n",
      " list([5, 32, 20, 104, 120, 0, 0, 33])\n",
      " list([5, 32, 20, 104, 120, 0, 0, 33]) list([21, 0, 0, 0, 0, 0, 0, 0])\n",
      " list([21, 0, 0, 0, 0, 0, 0, 0]) list([21, 0, 0, 0, 0, 0, 0, 0])\n",
      " list([21, 0, 0, 0, 0, 0, 0, 0]) list([21, 0, 0, 0, 0, 0, 0, 0])\n",
      " list([21, 0, 0, 0, 0, 0, 0, 0]) list([0, 32, 0, 0, 0, 0, 0, 0])\n",
      " list([0, 32, 0, 0, 0, 0, 0, 0]) list([0, 32, 0, 0, 0, 0, 0, 0])\n",
      " list([0, 32, 0, 0, 0, 0, 0, 0]) list([0, 32, 0, 0, 0, 0, 0, 0])\n",
      " list([0, 32, 0, 0, 0, 0, 0, 0]) list([16, 64, 96, 255, 125, 140, 9, 0])\n",
      " list([16, 64, 96, 255, 125, 140, 9, 0])\n",
      " list([16, 64, 96, 255, 125, 140, 9, 0])\n",
      " list([16, 64, 96, 255, 125, 140, 9, 0])\n",
      " list([16, 64, 96, 255, 125, 140, 9, 0])\n",
      " list([16, 64, 96, 255, 125, 140, 9, 0])\n",
      " list([255, 0, 0, 0, 255, 140, 9, 0]) list([255, 0, 0, 0, 255, 140, 9, 0])\n",
      " list([255, 0, 0, 0, 255, 140, 9, 0]) list([255, 0, 0, 0, 255, 140, 9, 0])\n",
      " list([255, 0, 0, 0, 255, 140, 9, 0]) list([255, 0, 0, 0, 255, 140, 9, 0])\n",
      " list([0, 0, 0, 128, 0, 103, 209, 19])\n",
      " list([0, 0, 0, 128, 0, 103, 209, 19])\n",
      " list([0, 0, 0, 128, 0, 103, 209, 19])\n",
      " list([0, 0, 0, 128, 0, 103, 209, 19])\n",
      " list([0, 0, 0, 128, 0, 103, 209, 19])\n",
      " list([0, 0, 0, 128, 0, 103, 209, 19])\n",
      " list([5, 33, 40, 10, 33, 30, 0, 111])\n",
      " list([5, 33, 40, 10, 33, 30, 0, 111])\n",
      " list([5, 33, 40, 10, 33, 30, 0, 111])\n",
      " list([5, 33, 40, 10, 33, 30, 0, 111])\n",
      " list([5, 33, 40, 10, 33, 30, 0, 111])\n",
      " list([5, 33, 40, 10, 33, 30, 0, 111]) list([254, 87, 0, 0, 0, 65, 0, 0])\n",
      " list([254, 87, 0, 0, 0, 65, 0, 0]) list([254, 87, 0, 0, 0, 65, 0, 0])\n",
      " list([254, 87, 0, 0, 0, 65, 0, 0]) list([254, 87, 0, 0, 0, 65, 0, 0])\n",
      " list([254, 87, 0, 0, 0, 65, 0, 0]) list([24, 33, 33, 48, 8, 143, 113, 5])\n",
      " list([24, 33, 33, 48, 8, 143, 113, 5])\n",
      " list([24, 33, 33, 48, 8, 143, 113, 5])\n",
      " list([24, 33, 33, 48, 8, 143, 113, 5])\n",
      " list([24, 33, 33, 48, 8, 143, 113, 5])\n",
      " list([24, 33, 33, 48, 8, 143, 113, 5])\n",
      " list([4, 0, 153, 29, 151, 2, 189, 0])\n",
      " list([4, 0, 153, 29, 151, 2, 189, 0])\n",
      " list([4, 0, 153, 29, 151, 2, 189, 0])\n",
      " list([4, 0, 153, 29, 151, 2, 189, 0])]\n",
      "Data output [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Aggregating data: Done\n",
      "#Normal:  0\n",
      "#Attack:  6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>header</th>\n",
       "      <th>payload</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1478195721.903877, 1478195721.903877, 1478195...</td>\n",
       "      <td>[[0, 5, 4, 5], [0, 5, 4, 5], [0, 5, 4, 5], [0,...</td>\n",
       "      <td>[[216, 0, 0, 138, 0, 0, 0, 0], [216, 0, 0, 138...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1478195721.91337, 1478195721.91337, 147819572...</td>\n",
       "      <td>[[0, 2, 10, 0], [0, 2, 10, 0], [0, 3, 2, 9], [...</td>\n",
       "      <td>[[4, 0, 153, 29, 151, 2, 189, 0], [4, 0, 153, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1478195721.921601, 1478195721.921601, 1478195...</td>\n",
       "      <td>[[0, 4, 3, 15], [0, 4, 3, 15], [0, 4, 3, 15], ...</td>\n",
       "      <td>[[16, 64, 96, 255, 125, 139, 9, 0], [16, 64, 9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1478195721.93064, 1478195721.93064, 147819572...</td>\n",
       "      <td>[[0, 3, 7, 0], [0, 3, 7, 0], [0, 3, 7, 0], [0,...</td>\n",
       "      <td>[[0, 32, 0, 0, 0, 0, 0, 0], [0, 32, 0, 0, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1478195721.939658, 1478195721.939658, 1478195...</td>\n",
       "      <td>[[0, 3, 5, 0], [0, 3, 5, 0], [0, 3, 7, 0], [0,...</td>\n",
       "      <td>[[5, 32, 68, 104, 120, 0, 0, 113], [5, 32, 68,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1478195721.948674, 1478195721.948674, 1478195...</td>\n",
       "      <td>[[0, 1, 5, 3], [0, 1, 5, 3], [0, 1, 5, 3], [0,...</td>\n",
       "      <td>[[0, 33, 16, 255, 0, 255, 0, 0], [0, 33, 16, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           timestamp  \\\n",
       "0  [1478195721.903877, 1478195721.903877, 1478195...   \n",
       "1  [1478195721.91337, 1478195721.91337, 147819572...   \n",
       "2  [1478195721.921601, 1478195721.921601, 1478195...   \n",
       "3  [1478195721.93064, 1478195721.93064, 147819572...   \n",
       "4  [1478195721.939658, 1478195721.939658, 1478195...   \n",
       "5  [1478195721.948674, 1478195721.948674, 1478195...   \n",
       "\n",
       "                                              header  \\\n",
       "0  [[0, 5, 4, 5], [0, 5, 4, 5], [0, 5, 4, 5], [0,...   \n",
       "1  [[0, 2, 10, 0], [0, 2, 10, 0], [0, 3, 2, 9], [...   \n",
       "2  [[0, 4, 3, 15], [0, 4, 3, 15], [0, 4, 3, 15], ...   \n",
       "3  [[0, 3, 7, 0], [0, 3, 7, 0], [0, 3, 7, 0], [0,...   \n",
       "4  [[0, 3, 5, 0], [0, 3, 5, 0], [0, 3, 7, 0], [0,...   \n",
       "5  [[0, 1, 5, 3], [0, 1, 5, 3], [0, 1, 5, 3], [0,...   \n",
       "\n",
       "                                             payload  \\\n",
       "0  [[216, 0, 0, 138, 0, 0, 0, 0], [216, 0, 0, 138...   \n",
       "1  [[4, 0, 153, 29, 151, 2, 189, 0], [4, 0, 153, ...   \n",
       "2  [[16, 64, 96, 255, 125, 139, 9, 0], [16, 64, 9...   \n",
       "3  [[0, 32, 0, 0, 0, 0, 0, 0], [0, 32, 0, 0, 0, 0...   \n",
       "4  [[5, 32, 68, 104, 120, 0, 0, 113], [5, 32, 68,...   \n",
       "5  [[0, 33, 16, 255, 0, 255, 0, 0], [0, 33, 16, 2...   \n",
       "\n",
       "                                               label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_data(attack, dataset_path, window_size = 100, strided_size = 100):\n",
    "    file_name = '{}{}_dataset.csv'.format(dataset_path, attack)\n",
    "    if not os.path.exists(file_name):\n",
    "            print(file_name, ' does not exist!')\n",
    "            return None\n",
    "    \n",
    "    splited_list = list()\n",
    "    \n",
    "    # for idx in range(0, )\n",
    "    \n",
    "    df = pd.read_csv(file_name, header=None, names=attributes)\n",
    "    print(\"Reading {}: done\".format(file_name))\n",
    "    df = df.sort_values('Timestamp', ascending=True)\n",
    "    df = df.swifter.apply(fill_flag, axis=1) \n",
    "    \n",
    "    num_data_bytes = 8\n",
    "    for x in range(num_data_bytes):\n",
    "        df['Data'+str(x)] = df['Data'+str(x)].map(lambda x: int(x, 16), na_action='ignore')\n",
    "    # print(\"HEADER BEFORE: \", header)\n",
    "    ## GET PAYLOAD \n",
    "    df['canID'] = df['canID'].apply(lambda x: hex_string_to_array(x))\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    data_cols = ['Data{}'.format(x) for x in range(num_data_bytes)]\n",
    "    df[data_cols] = df[data_cols].astype(int) \n",
    "    df['Data'] = df[data_cols].values.tolist()\n",
    "    df['Flag'] = df['Flag'].apply(lambda x: 1 if x=='T' else 0)\n",
    "    print(\"Pre-processing: Done\")\n",
    "    \n",
    "    print(\"BYTEs: \", df['Flag'][0].nbytes)\n",
    "    \n",
    "    as_strided = np.lib.stride_tricks.as_strided\n",
    "    output_shape = ((len(df) - window_size) // strided_size + 1, window_size)\n",
    "    timestamp = as_strided(df.Timestamp, output_shape, (8*strided_size, 8))\n",
    "    canid = as_strided(df.canID, output_shape, (8*strided_size, 8))\n",
    "    data = as_strided(df.Data, output_shape, (8*strided_size, 8)) #Stride is counted by bytes\n",
    "    label = as_strided(df.Flag, output_shape, (1*strided_size, 1))\n",
    "    \n",
    "    print(\"Data output\", timestamp[0])\n",
    "    print(\"Data output\", canid[0])\n",
    "    print(\"Data output\", data[0])\n",
    "    print(\"Data output\", label[0])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': pd.Series(timestamp.tolist()), \n",
    "        'header': pd.Series(canid.tolist()), \n",
    "        'payload': pd.Series(data.tolist()),\n",
    "        'label': pd.Series(label.tolist())\n",
    "    }, index= range(len(canid)))\n",
    "    \n",
    "    #df['label'] = df['label'].apply(lambda x: attack_id if any(x) else 0)\n",
    "    print(\"Aggregating data: Done\")\n",
    "    print('#Normal: ', df[df['label'] == 0].shape[0])\n",
    "    print('#Attack: ', df[df['label'] != 0].shape[0])\n",
    "    return df[['timestamp', 'header', 'payload', 'label']].reset_index().drop(['index'], axis=1)\n",
    "\n",
    "\n",
    "attributes = ['Timestamp', 'canID', 'DLC',\n",
    "                           'Data0', 'Data1', 'Data2',\n",
    "                           'Data3', 'Data4', 'Data5',\n",
    "                           'Data6', 'Data7', 'Flag']\n",
    "dataset_path  = '../data/car-hacking/'\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM', 'Test']\n",
    "\n",
    "split_data(attack_types[4], dataset_path, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(data, filename):\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        X = (row['timestamp'], row['header'], row['payload'])\n",
    "        Y = row['label']\n",
    "        tfrecord_writer.write(serialize_example(X, Y))\n",
    "    tfrecord_writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write_tfrecord() missing 2 required positional arguments: 'data' and 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m write_tfrecord()\n",
      "\u001b[0;31mTypeError\u001b[0m: write_tfrecord() missing 2 required positional arguments: 'data' and 'filename'"
     ]
    }
   ],
   "source": [
    "write_tfrecord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataPreprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataPreprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetPreprocess\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms, datasets\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataPreprocess'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
