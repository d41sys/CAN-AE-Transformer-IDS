{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 21:10:11.954884: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-08 21:10:12.006374: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-08 21:10:12.007580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 21:10:12.849151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import vaex\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import _warnings\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "attributes = ['Timestamp', 'canID', 'DLC', \n",
    "                           'Data0', 'Data1', 'Data2', \n",
    "                           'Data3', 'Data4', 'Data5', \n",
    "                           'Data6', 'Data7', 'Flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/hdd2/transformer-entropy-ids/notebooks', '/home/tiendat/miniconda3/envs/piptorchtf/lib/python39.zip', '/home/tiendat/miniconda3/envs/piptorchtf/lib/python3.9', '/home/tiendat/miniconda3/envs/piptorchtf/lib/python3.9/lib-dynload', '', '/home/tiendat/miniconda3/envs/piptorchtf/lib/python3.9/site-packages', '../']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from dataPreprocess import DatasetPreprocess\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report, confusion_matrix\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(label_y, pre_y, path):\n",
    "    confusion = confusion_matrix(label_y, pre_y)\n",
    "    print(confusion)\n",
    "\n",
    "def write_result(label_y, pre_y, classes_num):\n",
    "    if classes_num > 2:\n",
    "        accuracy = accuracy_score(label_y, pre_y)\n",
    "        macro_precision = precision_score(label_y, pre_y, average='macro')\n",
    "        macro_recall = recall_score(label_y, pre_y, average='macro')\n",
    "        macro_f1 = f1_score(label_y, pre_y, average='macro')\n",
    "        micro_precision = precision_score(label_y, pre_y, average='micro')\n",
    "        micro_recall = recall_score(label_y, pre_y, average='micro')\n",
    "        micro_f1 = f1_score(label_y, pre_y, average='micro')\n",
    "        print('  -- test result: ')\n",
    "        print('    -- accuracy: ', accuracy)\n",
    "        print('    -- macro precision: ', macro_precision)\n",
    "        print('    -- macro recall: ', macro_recall)\n",
    "        print('    -- macro f1 score: ', macro_f1)\n",
    "        print('    -- micro precision: ', micro_precision)\n",
    "        print('    -- micro recall: ', micro_recall)\n",
    "        print('    -- micro f1 score: ', micro_f1)\n",
    "        report = classification_report(label_y, pre_y)\n",
    "        print(report)\n",
    "    else:\n",
    "        accuracy = accuracy_score(label_y, pre_y)\n",
    "        precision = precision_score(label_y, pre_y)\n",
    "        recall = recall_score(label_y, pre_y)\n",
    "        f1 = f1_score(label_y, pre_y)\n",
    "        print('  -- test result: ')\n",
    "        print('    -- accuracy: ', accuracy)\n",
    "        print('    -- recall: ', recall)\n",
    "        print('    -- precision: ', precision)\n",
    "        print('    -- f1 score: ', f1)\n",
    "        report = classification_report(label_y, pre_y)\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Transformer'\n",
    "        self.slide_window = 1\n",
    "        self.slsum_count = 8 #int(math.pow(16, self.slide_window))  # 滑动窗口计数的特征的长度 n-gram?\n",
    "        self.dnn_out_d = 8 # 经过DNN后的滑动窗口计数特征的维度 Dimensions of sliding window count features after DNN 8\n",
    "        self.head_dnn_out_d = 32 \n",
    "        self.d_model = self.dnn_out_d + self.head_dnn_out_d # transformer的输入的特征的维度, dnn_out_d + 包头长度 The dimension of the input feature of the transformer, dnn_out_d + header length\n",
    "        self.pad_size = 29\n",
    "        self.window_size = 29\n",
    "        self.max_time_position = 10000\n",
    "        self.nhead = 5 # ori: 5\n",
    "        self.num_layers = 5\n",
    "        self.gran = 1e-6\n",
    "        self.log_e = 2\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.classes_num = 5 \n",
    "        self.batch_size = 10\n",
    "        self.epoch_num = 20\n",
    "        self.lr = 0.001 #0.00001 learning rate \n",
    "        self.train_pro = 0.7  # 训练集比例 Ratio of training set\n",
    "\n",
    "        self.root_dir = '../road/timesmooth/TFRecord_w29_s29/1/'\n",
    "        self.model_save_path = '../model/' + self.model_name + '/'\n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.mkdir(self.model_save_path)\n",
    "        self.result_file = '/mnt/hdd2/transformer-entropy-ids/result/trans8_performance.txt'\n",
    "\n",
    "        self.isload_model = False  # 是否加载模型继续训练 Whether to load the model and continue training\n",
    "        self.start_epoch = 18  # 加载的模型的epoch The epoch of the loaded model\n",
    "        # self.model_path = '../model/' + self.model_name + '/' + self.model_name + '_model_' + str(self.start_epoch) + '.pth'  # 要使用的模型的路径 path to the model to use\n",
    "        self.model_path = '../model/Transformer/best_model.pth'\n",
    "        \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, d_in, d_out):  # config.slsum_count, config.dnn_out_d\n",
    "        super(DNN, self).__init__()\n",
    "        self.l1 = nn.Linear(d_in, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('x: ', x.numpy()[0])\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = F.relu(self.l3(out))\n",
    "        # print('dnn out: ', out.detach().numpy()[0])\n",
    "        return out\n",
    "\n",
    "class Time_Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, max_time_position, device):\n",
    "        super(Time_Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, time_position):\n",
    "        out = x.permute(1, 0, 2)\n",
    "        out = out + nn.Parameter(time_position, requires_grad=False).to(self.device)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        return out\n",
    "\n",
    "class MyTrans(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MyTrans, self).__init__()\n",
    "        self.dnn = DNN(config.slsum_count, config.dnn_out_d).to(config.device)\n",
    "        self.head_dnn = DNN(4, config.head_dnn_out_d).to(config.device)\n",
    "        self.position_embedding = Time_Positional_Encoding(config.d_model, config.max_time_position, config.device).to(config.device)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=config.d_model, nhead=config.nhead).to(config.device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=config.num_layers).to(config.device)\n",
    "        self.fc = nn.Linear(config.d_model, config.classes_num).to(config.device)\n",
    "        self.pad_size = config.pad_size\n",
    "        self.dnn_out_d = config.dnn_out_d\n",
    "        self.head_dnn_out_d = config.head_dnn_out_d\n",
    "\n",
    "    def forward(self, header, sl_sum, mask, time_position):\n",
    "        dnn_out = torch.empty((sl_sum.shape[0], self.dnn_out_d, 0)).to(config.device)\n",
    "\n",
    "        for i in range(self.pad_size):\n",
    "            tmp = self.dnn(sl_sum[:, i, :]).unsqueeze(2)\n",
    "            dnn_out = torch.concat((dnn_out, tmp), dim=2)\n",
    "        dnn_out = dnn_out.permute(0, 2, 1)\n",
    "\n",
    "        head_dnn_out = torch.empty((header.shape[0], self.head_dnn_out_d, 0)).to(config.device)\n",
    "        for i in range(self.pad_size):\n",
    "            tmp = self.head_dnn(header[:, i, :]).unsqueeze(2)\n",
    "            head_dnn_out = torch.concat((head_dnn_out, tmp), dim=2)\n",
    "        head_dnn_out = head_dnn_out.permute(0, 2, 1)\n",
    "\n",
    "        x = torch.concat((head_dnn_out, dnn_out), dim=2).permute(1, 0, 2)\n",
    "\n",
    "        out = self.position_embedding(x, time_position)\n",
    "        out = self.transformer_encoder(out, src_key_padding_mask=mask)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = torch.sum(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "../model/Transformer/best_model.pth\n",
      "finish load data\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "print(config.device)\n",
    "print(config.model_path)\n",
    "\n",
    "# seed = 1\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "train_dataset = DatasetPreprocess(config.root_dir, config.window_size, config.pad_size, config.d_model, config.max_time_position, config.gran, config.log_e, is_train=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = DatasetPreprocess(config.root_dir, config.window_size, config.pad_size, config.d_model, config.max_time_position, config.gran, config.log_e, is_train=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=config.batch_size)\n",
    "print('finish load data')\n",
    "\n",
    "# model = torch.load(config.model_path)\n",
    "# print(\"Model loaded\")\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss().to(config.device)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m linked_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m j, train_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      5\u001b[0m         \u001b[39m# print(f\"Origin time: {train_batch['oritime']} and length: {len(train_batch['oritime'])}\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[39m# aa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         canid \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m         message \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/hdd2/transformer-entropy-ids/notebooks/../dataPreprocess.py:54\u001b[0m, in \u001b[0;36mDatasetPreprocess.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m dataset \u001b[39m=\u001b[39m TFRecordDataset(filenames, index_path, description)\n\u001b[1;32m     53\u001b[0m dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n\u001b[1;32m     56\u001b[0m \u001b[39m# data['timestamp'] = data['timestamp'] - data['timestamp'].min()\u001b[39;00m\n\u001b[1;32m     58\u001b[0m timestamp, header, payload, label \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mpayload\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/tfrecord/reader.py:223\u001b[0m, in \u001b[0;36mexample_loader\u001b[0;34m(data_path, index_path, description, shard, compression_type)\u001b[0m\n\u001b[1;32m    220\u001b[0m example \u001b[39m=\u001b[39m example_pb2\u001b[39m.\u001b[39mExample()\n\u001b[1;32m    221\u001b[0m example\u001b[39m.\u001b[39mParseFromString(record)\n\u001b[0;32m--> 223\u001b[0m \u001b[39myield\u001b[39;00m extract_feature_dict(example\u001b[39m.\u001b[39;49mfeatures, description, typename_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/tfrecord/reader.py:156\u001b[0m, in \u001b[0;36mextract_feature_dict\u001b[0;34m(features, description, typename_mapping)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m all_keys:\n\u001b[1;32m    154\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist (select from \u001b[39m\u001b[39m{\u001b[39;00mall_keys\u001b[39m}\u001b[39;00m\u001b[39m)!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m     processed_features[key] \u001b[39m=\u001b[39m get_value(typename, typename_mapping, key)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m processed_features\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/tfrecord/reader.py:137\u001b[0m, in \u001b[0;36mextract_feature_dict.<locals>.get_value\u001b[0;34m(typename, typename_mapping, key)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_value\u001b[39m(typename, typename_mapping, key):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m process_feature(features[key], typename,\n\u001b[1;32m    138\u001b[0m                            typename_mapping, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/piptorchtf/lib/python3.9/site-packages/tfrecord/reader.py:120\u001b[0m, in \u001b[0;36mprocess_feature\u001b[0;34m(feature, typename, typename_mapping, key)\u001b[0m\n\u001b[1;32m    118\u001b[0m     value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(value, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m inferred_typename \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint64_list\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(value, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mint64)\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "linked_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j, train_batch in enumerate(train_loader):\n",
    "        # print(f\"Origin time: {train_batch['oritime']} and length: {len(train_batch['oritime'])}\")\n",
    "        # aa\n",
    "        canid = []\n",
    "        message = []\n",
    "        time_stamp = [] \n",
    "        \n",
    "        for index in range(len(train_batch['oritime'])): \n",
    "            time_stamp.extend(train_batch['oritime'][index].tolist())\n",
    "        \n",
    "            for msg in train_batch['header'][index]:\n",
    "                # print(f\"{msg}\")\n",
    "                canid.append(''.join(map(str, msg.numpy())))\n",
    "                # print(f\"{''.join(map(str, msg))}\")\n",
    "                \n",
    "            for msg in train_batch['payload'][index]:\n",
    "                # print(f\"{msg}\")\n",
    "                message.append(msg.numpy())\n",
    "                # print(f\"{''.join(map(str, msg))}\")\n",
    "        # print(canid)\n",
    "        # print(len(message))\n",
    "        # print(len(time_stamp))\n",
    "        \n",
    "        # Populate the dictionary\n",
    "        for i, id in enumerate(canid):\n",
    "            if id in linked_dict:\n",
    "                linked_dict[id]['payload'].append(message[i])\n",
    "                linked_dict[id]['timestamp'].append(time_stamp[i])\n",
    "            else:\n",
    "                linked_dict[id] = {'payload': [message[i]], 'timestamp': [time_stamp[i]]}\n",
    "                \n",
    "for can_id in linked_dict:\n",
    "    sorted_indices = sorted(range(len(linked_dict[can_id]['timestamp'])), key=lambda k: linked_dict[can_id]['timestamp'][k])\n",
    "    linked_dict[can_id]['timestamp'] = [linked_dict[can_id]['timestamp'][i] for i in sorted_indices]\n",
    "    linked_dict[can_id]['payload'] = [linked_dict[can_id]['payload'][i] for i in sorted_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for can_id in linked_dict:\n",
    "    count+=1\n",
    "    time_diffs = [linked_dict[can_id]['timestamp'][i+1] - linked_dict[can_id]['timestamp'][i] for i in range(len(linked_dict[can_id]['timestamp']) - 1)]\n",
    "    max_time_diff = max(time_diffs)\n",
    "    min_time_diff = min(time_diffs)\n",
    "    linked_dict[can_id]['max_time'] = max_time_diff\n",
    "    linked_dict[can_id]['min_time'] = min_time_diff\n",
    "    # print(f\"CANID: {can_id} with max: {linked_dict[can_id]['max_time']} and min: {linked_dict[can_id]['min_time']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17563\n"
     ]
    }
   ],
   "source": [
    "[linked_dict['0230']['payload'][i] for i in range(20)]\n",
    "print(len(linked_dict['0230']['payload']))\n",
    "# for i in linked_dict['0230']['payload']:\n",
    "#     print(i.tolist())\n",
    "#     aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linked_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m             hamming_distances[(j, i)] \u001b[39m=\u001b[39m distance\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m hamming_distances, max_distance, min_distance\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m can_id \u001b[39min\u001b[39;00m linked_dict:\n\u001b[1;32m     25\u001b[0m     pair_distances, max_distance, min_distance \u001b[39m=\u001b[39m all_pair_hamming_distance_int_list(linked_dict[can_id][\u001b[39m'\u001b[39m\u001b[39mpayload\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[39m# print(f\"CANID: {can_id} with max: {linked_dict[can_id]['max_time']} and min: {linked_dict[can_id]['min_time']}\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linked_dict' is not defined"
     ]
    }
   ],
   "source": [
    "def hamming_distance_int_list(seq1, seq2):\n",
    "    # Calculate the Hamming distance between two binary sequences represented as lists\n",
    "    return sum(bin(x ^ y).count('1') for x, y in zip(seq1, seq2))\n",
    "\n",
    "def all_pair_hamming_distance_int_list(sequence_list):\n",
    "    \n",
    "    n = len(sequence_list)\n",
    "    hamming_distances = {}\n",
    "    max_distance = 0\n",
    "    min_distance = 64\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = hamming_distance_int_list(sequence_list[i].tolist(), sequence_list[j].tolist())\n",
    "            max_distance = max(max_distance, distance)\n",
    "            min_distance = min(min_distance, distance)\n",
    "            hamming_distances[(i, j)] = distance\n",
    "            # Since Hamming distance is symmetric (H(A, B) == H(B, A)),\n",
    "            # we can also store the distance for (j, i) pair.\n",
    "            hamming_distances[(j, i)] = distance\n",
    "    \n",
    "    return hamming_distances, max_distance, min_distance\n",
    "\n",
    "for can_id in linked_dict:\n",
    "    pair_distances, max_distance, min_distance = all_pair_hamming_distance_int_list(linked_dict[can_id]['payload'])\n",
    "    # print(f\"CANID: {can_id} with max: {linked_dict[can_id]['max_time']} and min: {linked_dict[can_id]['min_time']}\")\n",
    "    linked_dict[can_id]['max_hmc'] = max_distance\n",
    "    linked_dict[can_id]['min_hmc'] = min_distance\n",
    "    # for pair, distance in pair_distances.items():\n",
    "    #     idx1, idx2 = pair\n",
    "    #     print(f\"Hamming distance between {linked_dict[can_id]['payload'][idx1]} and {linked_dict[can_id]['payload'][idx2]}: {distance}\")\n",
    "    print(f\"CANID: {can_id} max distance: {max_distance} and min distance: {min_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['05141', '02811', '00107', '011210', '00154', '00014', '0585', '00130', '05113', '06914', '0125', '0193', '02014', '0522', '0366', '0580', '0434', '06140', '0498', '0230', '03213', '0153', '0662', '01912', '00130', '02141', '0354', '05141', '0033', '001110', '00014', '02142', '0345', '0430', '0130', '00130', '0033', '00130', '02141', '0354', '05141', '0162', '00107', '0497', '00154', '0125', '05113', '0193', '02014', '0522', '0366', '0580', '0434', '06140', '0498', '0230', '00014', '03121', '06140', '0354', '05141', '02811', '00130', '0033', '061512', '00107', '0239', '00130', '00014', '041513', '05113', '0107', '06914', '0193', '02014', '0522', '02141', '0125', '0354', '0153', '05141', '0662', '01912', '0033', '00130', '0274', '0366', '02811', '0580', '0434', '06140', '00107', '00313', '0498', '0230', '06914', '0125', '00014', '00130', '0033', '05113', '0107', '06140', '02141', '0354', '05141', '03121', '02121', '0162', '00107', '0125', '00014', '0153', '0295', '0662', '01912', '0033', '00014', '00130', '03121', '02141', '0125', '0354', '05141', '0162', '00107', '0107', '0497', '0193', '02014', '0522', '0366', '0580', '0434', '06140', '0498', '0230', '0153', '0295', '0662', '01912', '00130', '0033', '00014', '0274', '0193', '02014', '0522', '0366', '0580', '0434', '02141', '0354', '05141', '06140', '0162', '00107', '0498', '041211', '0280', '0230', '0153', '0662', '01912', '00130', '00014', '06914', '0125', '0033', '0274', '00120', '00130', '06612', '06140', '0295', '0662', '0684', '0636', '02137', '01912', '00130', '0033', '0274', '02141', '0354', '05141', '00120', '00014', '0125', '0162', '00107', '0497', '03144', '06512', '0577', '03121', '06140', '0107', '00130', '0033', '0354', '05141', '02811', '05141', '06914', '0125', '0162', '00107', '06140', '0153', '0662', '01912', '00130', '0107', '0033', '0274', '00120', '00154', '0354', '05141', '02811', '00014', '00107', '0125', '0407', '0193', '02014', '0522', '0366', '0580', '0434', '00130', '02811', '00014', '00107', '011210', '0125', '0193', '02014', '0522', '0366', '0580', '00130', '0033', '0434', '06140', '0498', '0230', '02141', '0354', '05141', '00014', '0162', '00107', '02103', '06914', '0125', '06140', '0153', '0295', '0662', '00120', '00014', '06612', '02614', '00137', '0498', '04129', '01136', '01104', '0230', '0354', '05141', '02811', '00107', '0407', '06140', '00130', '0033', '0125', '00014', '0107', '001110', '0297', '05111', '03213', '02141', '0354', '05141', '0162']\n",
    "\n",
    "\n",
    "[linked_dict['0230']['timestamp'][i] for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- test result: \n",
      "    -- accuracy:  0.9999883322054465\n",
      "    -- macro precision:  0.9999962205319601\n",
      "    -- macro recall:  0.9999719881743399\n",
      "    -- macro f1 score:  0.999984103808873\n",
      "    -- micro precision:  0.9999883322054465\n",
      "    -- micro recall:  0.9999883322054465\n",
      "    -- micro f1 score:  0.9999883322054465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    105833\n",
      "         1.0       1.00      1.00      1.00     11236\n",
      "         2.0       1.00      1.00      1.00     13346\n",
      "         3.0       1.00      1.00      1.00     19585\n",
      "         4.0       1.00      1.00      1.00     21412\n",
      "\n",
      "    accuracy                           1.00    171412\n",
      "   macro avg       1.00      1.00      1.00    171412\n",
      "weighted avg       1.00      1.00      1.00    171412\n",
      "\n",
      "[[105833      0      0      0      0]\n",
      " [     1  11235      0      0      0]\n",
      " [     0      0  13346      0      0]\n",
      " [     1      0      0  19584      0]\n",
      " [     0      0      0      0  21412]]\n"
     ]
    }
   ],
   "source": [
    "label_y = []\n",
    "pre_y = []\n",
    "timeCalc = [] \n",
    "with torch.no_grad():\n",
    "    for j, test_sample_batch in enumerate(test_loader):\n",
    "        start_time = time.time()\n",
    "        test_header = test_sample_batch['header'].type(torch.FloatTensor).to(config.device)\n",
    "        test_sl_sum = test_sample_batch['payload'].type(torch.FloatTensor).to(config.device)\n",
    "        test_mask = test_sample_batch['mask'].to(config.device)\n",
    "        test_label = test_sample_batch['label'].to(config.device)\n",
    "        test_time_position = test_sample_batch['time'].to(config.device)\n",
    "        \n",
    "        # print(f\"TEST HEADER: {test_sample_batch['header']}\")\n",
    "        # print(len(test_sample_batch['label']))\n",
    "        # aa\n",
    "        \n",
    "        # for session in test_sample_batch['header']:\n",
    "        #     for msg in session:\n",
    "        #         print(f\"Message: {''.join(map(str, msg.numpy()))}\")\n",
    "        #     aa \n",
    "        \n",
    "        test_out = model(test_header, test_sl_sum, test_mask, test_time_position)\n",
    "\n",
    "        pre = torch.max(test_out, 1)[1].cpu().numpy()\n",
    "        pre_y = np.concatenate([pre_y, pre], 0)\n",
    "        label_y = np.concatenate([label_y, test_label.cpu().numpy()], 0)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        timeCalc.append(execution_time)\n",
    "    write_result(label_y, pre_y, config.classes_num)\n",
    "    draw_confusion(label_y, pre_y, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_int(hex_value):\n",
    "    return int(hex_value, base=16)\n",
    "\n",
    "def hex_string_to_array(hex_string):\n",
    "    return ''.join(map(str, list(map(hex_to_int, hex_string))))\n",
    "\n",
    "def complete_field(sample):\n",
    "    if not isinstance(sample['Flag'], str):\n",
    "        col = 'Data' + str(sample['DLC'])\n",
    "        sample['Flag'] = sample[col]\n",
    "        sample[col] = '00'\n",
    "    return sample\n",
    "\n",
    "def max_min(lst):\n",
    "    \"\"\"\n",
    "    Finds the maximum and minimum values in a list.\n",
    "    Returns a tuple containing the maximum and minimum values.\n",
    "    \"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return None\n",
    "    \n",
    "    max_val = lst[0]\n",
    "    min_val = lst[0]\n",
    "    for val in lst:\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "        elif val < min_val:\n",
    "            min_val = val\n",
    "    \n",
    "    return (max_val, min_val)\n",
    "\n",
    "def extract_time_interval(time_list):\n",
    "    \"\"\"\n",
    "    Calculates the differences between adjacent elements in an ascending array.\n",
    "    Returns a list containing all differences.\n",
    "    \"\"\"\n",
    "    if len(time_list) < 2:\n",
    "        return None\n",
    "    \n",
    "    differences = []\n",
    "    for i in range(len(time_list)-1):\n",
    "        diff = time_list[i+1] - time_list[i]\n",
    "        differences.append(diff)\n",
    "    \n",
    "    return max_min(differences)\n",
    "\n",
    "def hamming_distance_int_list(seq1, seq2):\n",
    "    # Calculate the Hamming distance between two binary sequences represented as lists\n",
    "    return sum(bin(x ^ y).count('1') for x, y in zip(seq1, seq2))\n",
    "\n",
    "def all_pair_hamming_distance_int_list(sequence_list):\n",
    "    n = len(sequence_list)\n",
    "    hamming_distances = {}\n",
    "    max_distance = 0\n",
    "    min_distance = 64\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = hamming_distance_int_list(sequence_list[i], sequence_list[j])\n",
    "            max_distance = max(max_distance, distance)\n",
    "            min_distance = min(min_distance, distance)\n",
    "            hamming_distances[(i, j)] = distance\n",
    "            # Since Hamming distance is symmetric (H(A, B) == H(B, A)),\n",
    "            # we can also store the distance for (j, i) pair.\n",
    "            hamming_distances[(j, i)] = distance\n",
    "    \n",
    "    return hamming_distances, max_distance, min_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance between the integer lists: 2\n"
     ]
    }
   ],
   "source": [
    "def hamming_distance_int(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "    \n",
    "    distance = sum(a != b for a, b in zip(list1, list2))\n",
    "    return distance\n",
    "\n",
    "# Example usage:\n",
    "list1_example = [216, 0, 0, 137, 0, 0, 0, 0]\n",
    "list2_example = [216, 85, 0, 139, 0, 0, 0, 0]\n",
    "distance_value = hamming_distance_int(list1_example, list2_example)\n",
    "print(\"Hamming distance between the integer lists:\", distance_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 85, 0, 139, 0, 0, 0, 0]: 5\n",
      "Hamming distance between [216, 85, 0, 139, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 5\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 0, 0, 137, 0, 0, 0, 0]: 2\n",
      "Hamming distance between [216, 0, 0, 137, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 2\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 85, 0, 140, 0, 0, 0, 0]: 6\n",
      "Hamming distance between [216, 85, 0, 140, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 6\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 0\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 0\n",
      "Hamming distance between [216, 85, 0, 139, 0, 0, 0, 0] and [216, 0, 0, 137, 0, 0, 0, 0]: 5\n",
      "Hamming distance between [216, 0, 0, 137, 0, 0, 0, 0] and [216, 85, 0, 139, 0, 0, 0, 0]: 5\n",
      "Hamming distance between [216, 85, 0, 139, 0, 0, 0, 0] and [216, 85, 0, 140, 0, 0, 0, 0]: 3\n",
      "Hamming distance between [216, 85, 0, 140, 0, 0, 0, 0] and [216, 85, 0, 139, 0, 0, 0, 0]: 3\n",
      "Hamming distance between [216, 85, 0, 139, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 5\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 85, 0, 139, 0, 0, 0, 0]: 5\n",
      "Hamming distance between [216, 0, 0, 137, 0, 0, 0, 0] and [216, 85, 0, 140, 0, 0, 0, 0]: 6\n",
      "Hamming distance between [216, 85, 0, 140, 0, 0, 0, 0] and [216, 0, 0, 137, 0, 0, 0, 0]: 6\n",
      "Hamming distance between [216, 0, 0, 137, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 2\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 0, 0, 137, 0, 0, 0, 0]: 2\n",
      "Hamming distance between [216, 85, 0, 140, 0, 0, 0, 0] and [216, 0, 0, 138, 0, 0, 0, 0]: 6\n",
      "Hamming distance between [216, 0, 0, 138, 0, 0, 0, 0] and [216, 85, 0, 140, 0, 0, 0, 0]: 6\n",
      "Max distance: 6 and min distance: 0\n"
     ]
    }
   ],
   "source": [
    "def hamming_distance_int_list(seq1, seq2):\n",
    "    # Calculate the Hamming distance between two binary sequences represented as lists\n",
    "    return sum(bin(x ^ y).count('1') for x, y in zip(seq1, seq2))\n",
    "\n",
    "def all_pair_hamming_distance_int_list(sequence_list):\n",
    "    n = len(sequence_list)\n",
    "    hamming_distances = {}\n",
    "    max_distance = 0\n",
    "    min_distance = 64\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = hamming_distance_int_list(sequence_list[i], sequence_list[j])\n",
    "            max_distance = max(max_distance, distance)\n",
    "            min_distance = min(min_distance, distance)\n",
    "            hamming_distances[(i, j)] = distance\n",
    "            # Since Hamming distance is symmetric (H(A, B) == H(B, A)),\n",
    "            # we can also store the distance for (j, i) pair.\n",
    "            hamming_distances[(j, i)] = distance\n",
    "    \n",
    "    return hamming_distances, max_distance, min_distance\n",
    "\n",
    "# Example usage:\n",
    "binary_sequence_list_example = [\n",
    "    [216, 0, 0, 138, 0, 0, 0, 0],\n",
    "    [216, 85, 0, 139, 0, 0, 0, 0],\n",
    "    [216, 0, 0, 137, 0, 0, 0, 0],\n",
    "    [216, 85, 0, 140, 0, 0, 0, 0],\n",
    "    [216, 0, 0, 138, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "pair_distances, max_distance, min_distance = all_pair_hamming_distance_int_list(binary_sequence_list_example)\n",
    "\n",
    "# Print the Hamming distances for all pairs\n",
    "for pair, distance in pair_distances.items():\n",
    "    idx1, idx2 = pair\n",
    "    print(f\"Hamming distance between {binary_sequence_list_example[idx1]} and {binary_sequence_list_example[idx2]}: {distance}\")\n",
    "    \n",
    "print(f\"Max distance: {max_distance} and min distance: {min_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Entropy H(Y|X) = 0.9512050593046015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conditional_entropy(y, x):\n",
    "    # Convert input to numpy arrays if they are not already\n",
    "    y = np.array(y)\n",
    "    x = np.array(x)\n",
    "    \n",
    "    # Get unique values and their counts for X and Y\n",
    "    unique_x, counts_x = np.unique(x, return_counts=True)\n",
    "    unique_y, counts_y = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Initialize conditional entropy\n",
    "    h_y_given_x = 0.0\n",
    "    \n",
    "    # Calculate conditional entropy\n",
    "    for xi, cx in zip(unique_x, counts_x):\n",
    "        # Get the subset of Y corresponding to xi in X\n",
    "        y_given_xi = y[x == xi]\n",
    "        \n",
    "        # Get unique values and their counts for the subset of Y given xi in X\n",
    "        unique_y_given_xi, counts_y_given_xi = np.unique(y_given_xi, return_counts=True)\n",
    "        \n",
    "        # Calculate conditional probabilities P(Y|X) for xi\n",
    "        p_y_given_xi = counts_y_given_xi / cx\n",
    "        \n",
    "        # Calculate conditional entropy contribution for xi\n",
    "        h_y_given_xi = np.sum(-p_y_given_xi * np.log2(p_y_given_xi))\n",
    "        \n",
    "        # Weighted sum based on P(X=xi)\n",
    "        h_y_given_x += h_y_given_xi * (cx / len(x))\n",
    "    \n",
    "    return h_y_given_x\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X and Y are two discrete random variables in the form of lists or arrays\n",
    "X = [1, 1, 1, 0, 1, 1, 0, 0]\n",
    "Y = [1, 0, 0, 1, 0, 1, 0, 0]\n",
    "\n",
    "# Calculate H(Y|X)\n",
    "conditional_entropy_value = conditional_entropy(Y, X)\n",
    "print(\"Conditional Entropy H(Y|X) =\", conditional_entropy_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5367431640625e-07"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01722097396850586-0.017220020294189453"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piptorchtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
