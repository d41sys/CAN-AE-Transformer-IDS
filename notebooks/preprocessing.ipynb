{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T01:46:07.333489Z",
     "start_time": "2023-05-23T01:46:04.059712Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import vaex\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T01:46:07.338945Z",
     "start_time": "2023-05-23T01:46:07.334460Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.11.0\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T01:46:07.362394Z",
     "start_time": "2023-05-23T01:46:07.337583Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_flag(sample):\n",
    "    if not isinstance(sample['Flag'], str):\n",
    "        col = 'Data' + str(sample['DLC'])\n",
    "        sample['Flag'] = sample[col]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T01:46:07.379269Z",
     "start_time": "2023-05-23T01:46:07.340317Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_canid_bits(cid):\n",
    "    try:\n",
    "        s = bin(int(str(cid), 16))[2:].zfill(29)\n",
    "        bits = list(map(int, list(s)))\n",
    "        return bits\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T01:46:07.386962Z",
     "start_time": "2023-05-23T01:46:07.362612Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/car-hacking/Fuzzy_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Read by dask first\n",
    "attributes = ['Timestamp', 'canID', 'DLC',\n",
    "                           'Data0', 'Data1', 'Data2',\n",
    "                           'Data3', 'Data4', 'Data5',\n",
    "                           'Data6', 'Data7', 'Flag']\n",
    "dataset_path  = '../data/car-hacking/'\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "attack = attack_types[0]\n",
    "file_name = '{}{}_dataset.csv'.format(dataset_path, attack)\n",
    "print(file_name)\n",
    "# df = pd.read_csv(file_name, header=None, names=attributes)\n",
    "# for f in files[1]:\n",
    "#     print('Reading file: ', f)\n",
    "#     df = df.append(pd.read_csv(f, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                    0       1      2       3       4      5       6       7       8        9       10      11\n",
      "npartitions=3                                                                                                \n",
      "               float64  object  int64  object  object  int64  object  object  object  float64  object  object\n",
      "                   ...     ...    ...     ...     ...    ...     ...     ...     ...      ...     ...     ...\n",
      "                   ...     ...    ...     ...     ...    ...     ...     ...     ...      ...     ...     ...\n",
      "                   ...     ...    ...     ...     ...    ...     ...     ...     ...      ...     ...     ...\n",
      "Dask Name: read-csv, 1 graph layer\n"
     ]
    }
   ],
   "source": [
    "df2 = dd.read_csv(file_name, header=None)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:16:01.104458Z",
     "start_time": "2023-05-09T01:16:01.093814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(file_name):\n",
    "    # Read CSV file\n",
    "    df = dd.read_csv(file_name, header=None, names=attributes, dtype={'Data1': 'object', 'Data2': 'object', 'Data4': 'object',\n",
    "       'Data6': 'object'}) # Data2 & Data 6 is object - something field in hex can't convert to int or float\n",
    "    print('Reading from {}: DONE'.format(file_name))\n",
    "    print('Dask processing: -------------')\n",
    "    df = df.apply(fill_flag, axis=1, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'int64', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'float64', 'Data7': 'object', 'Flag': 'object'})\n",
    "    pd_df = df.compute()\n",
    "    pd_df = pd_df[['Timestamp', 'canID', 'Flag']].sort_values('Timestamp',  ascending=True)\n",
    "    pd_df['canBits'] = pd_df.canID.apply(convert_canid_bits)\n",
    "    pd_df['Flag'] = pd_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "    print('Dask processing: DONE')\n",
    "    print('Aggregate data -----------------')\n",
    "    as_strided = np.lib.stride_tricks.as_strided\n",
    "    win = 29\n",
    "    s = 29\n",
    "    feature = as_strided(pd_df.canBits, ((len(pd_df) - win) // s + 1, win), (8*s, 8)) #Stride is counted by bytes\n",
    "    label = as_strided(pd_df.Flag, ((len(pd_df) - win) // s + 1, win), (1*s, 1))\n",
    "    df = pd.DataFrame({\n",
    "        'features': pd.Series(feature.tolist()),\n",
    "        'label': pd.Series(label.tolist())\n",
    "    }, index= range(len(feature)))\n",
    "\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if any(x) else 0)\n",
    "    print('Preprocessing: DONE')\n",
    "    print('#Normal: ', df[df['label'] == 0].shape[0])\n",
    "    print('#Attack: ', df[df['label'] == 1].shape[0])\n",
    "    return df[['features', 'label']].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T05:58:33.991136Z",
     "start_time": "2023-05-23T05:56:15.593611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from ../data/car-hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = dd.read_csv(file_name, header=None, names=attributes, dtype={'Data2': 'object',\n",
    "       'Data6': 'object'})\n",
    "print('Reading from {}: DONE'.format(file_name))\n",
    "print('Dask processing: -------------')\n",
    "df = df.apply(fill_flag, axis=1, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'int64', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'float64', 'Data7': 'object', 'Flag': 'object'})\n",
    "pd_df = df.compute()\n",
    "pd_df = pd_df[['Timestamp', 'canID', 'Flag', 'DLC']].sort_values('Timestamp',  ascending=True)\n",
    "#pd_df['canBits'] = pd_df.canID.apply(convert_canid_bits)\n",
    "pd_df['Flag'] = pd_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "filtered_df = pd_df[pd_df['Flag'] == False]\n",
    "\n",
    "canIDs = []\n",
    "for index, row in pd_df.iterrows():\n",
    "    canIDs.append(row['canID'])\n",
    "\n",
    "print(canIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:14.558290Z",
     "start_time": "2023-05-09T01:17:36.098764Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from ../data/car-hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  87888\n",
      "#Attack:  44486\n"
     ]
    }
   ],
   "source": [
    "df = preprocess(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:14.820518Z",
     "start_time": "2023-05-09T01:19:14.687965Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1              NaN\n",
       "19        0.009996\n",
       "42        0.010007\n",
       "60        0.009996\n",
       "81        0.010007\n",
       "            ...   \n",
       "338341    1.865040\n",
       "341957    2.013793\n",
       "353435    6.547768\n",
       "363092    5.330678\n",
       "367266    3.116163\n",
       "Name: Timestamp, Length: 53477, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df[pd_df['canID'] == '02b0']['Timestamp'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:14.945067Z",
     "start_time": "2023-05-09T01:19:14.821952Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Normal: 3347013\n",
      "#Attack: 491847\n"
     ]
    }
   ],
   "source": [
    "print('#Normal:', pd_df[pd_df['Flag'] == False].shape[0])\n",
    "print('#Attack:', pd_df[pd_df['Flag'] == True].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.440342Z",
     "start_time": "2023-05-09T01:19:15.025088Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "as_strided = np.lib.stride_tricks.as_strided\n",
    "win = 29\n",
    "s = 29\n",
    "feature = as_strided(pd_df.canBits, ((len(pd_df) - win) // s + 1, win), (8*s, 8))\n",
    "label = as_strided(pd_df.Flag, ((len(pd_df) - win) // s + 1, win), (1*s, 1))\n",
    "df = pd.DataFrame({\n",
    "    'features': pd.Series(feature.tolist()),\n",
    "    'label': pd.Series(label.tolist())\n",
    "}, index= range(len(feature)))\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: 1 if any(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.447231Z",
     "start_time": "2023-05-09T01:19:15.441072Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Normal:  87888\n",
      "#Attack:  44486\n"
     ]
    }
   ],
   "source": [
    "print('#Normal: ', df[df['label'] == 0].shape[0])\n",
    "print('#Attack: ', df[df['label'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.450275Z",
     "start_time": "2023-05-09T01:19:15.448780Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serialize_example(x, y):\n",
    "    \"\"\"converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    #Need to pay attention to whether it needs to be converted to numpy() form\n",
    "    input_features = tf.train.Int64List(value = np.array(x).flatten())\n",
    "    label = tf.train.Int64List(value = np.array([y]))\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(int64_list = input_features),\n",
    "            \"label\" : tf.train.Feature(int64_list = label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.454360Z",
     "start_time": "2023-05-09T01:19:15.453037Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_tfrecord(example):\n",
    "    input_dim = 841\n",
    "    feature_description = {\n",
    "    'input_features': tf.io.FixedLenFeature([input_dim], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([1], tf.int64)\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.457542Z",
     "start_time": "2023-05-09T01:19:15.456198Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_from_tfrecord(tf_filepath, batch_size, repeat_time):\n",
    "    data = tf.data.TFRecordDataset(tf_filepath)\n",
    "    data = data.map(read_tfrecord)\n",
    "    data = data.shuffle(2)\n",
    "    data = data.repeat(repeat_time + 1)\n",
    "    data = data.batch(batch_size)\n",
    "    # print(tf.data.experimental.cardinality(data))\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.479015Z",
     "start_time": "2023-05-09T01:19:15.460509Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_helper(data_tf, sess):\n",
    "    n_labels = 2\n",
    "    data = sess.run(data_tf)\n",
    "    x, y = data['input_features'], data['label']\n",
    "    size = x.shape[0]\n",
    "    y_one_hot = np.eye(n_labels)[y].reshape([size, n_labels])\n",
    "    return x, y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.488044Z",
     "start_time": "2023-05-09T01:19:15.463523Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_size(file_path):\n",
    "    dataset = data_from_tfrecord(file_path, 1000, 0)\n",
    "    # print(tf.data.experimental.cardinality(dataset).numpy())\n",
    "    init = tf.global_variables_initializer()\n",
    "    size = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        while True:\n",
    "            try:\n",
    "                x_l, y_l = data_helper(dataset, sess)\n",
    "                size += x_l.shape[0]\n",
    "            except Exception as e:\n",
    "                print(type(e).__name__)\n",
    "                break\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T05:10:54.069106Z",
     "start_time": "2023-05-09T05:10:54.061622Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_tfrecord(data, filename):\n",
    "    print('Writing {}================= '.format(filename))\n",
    "    print(\"Debug=================\")\n",
    "    print(data)\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        tfrecord_writer.write(serialize_example(row['features'], row['label']))\n",
    "    tfrecord_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.488602Z",
     "start_time": "2023-05-09T01:19:15.471519Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(source_path, dest_path, DATASET_SIZE,\\\n",
    "                     train_size = 500 * 1000, train_label_size = 100 * 1000):\n",
    "    # dataset = data_from_tfrecord('./Data/TFRecord/DoS', 1000, 0)\n",
    "    #DATASET_SIZE = data_info['./Data/TFRecord/DoS']\n",
    "    #train_size = 500 * 1000\n",
    "    #train_label_size = 100 * 1000\n",
    "    val_size = int((DATASET_SIZE - train_size) * 0.2)\n",
    "    test_size = DATASET_SIZE - train_size - val_size\n",
    "\n",
    "    print(train_size, val_size, test_size)\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(source_path)\n",
    "    dataset = dataset.shuffle(1000000)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "\n",
    "    train = dataset.take(train_size)\n",
    "    train_label = train.take(train_label_size)\n",
    "    train_unlabel = train.skip(train_label_size)\n",
    "\n",
    "    val = dataset.skip(train_size)\n",
    "    test = val.skip(val_size)\n",
    "    val = val.take(val_size)\n",
    "\n",
    "    batch_size = 10000\n",
    "    train_label = train_label.batch(batch_size)\n",
    "    train_unlabel = train_unlabel.batch(batch_size)\n",
    "    test = test.batch(batch_size)\n",
    "    val = val.batch(batch_size)\n",
    "\n",
    "    train_test_info = {\n",
    "        \"train_unlabel\": train_size - train_label_size,\n",
    "        \"train_label\": train_label_size,\n",
    "        \"validation\": val_size,\n",
    "        \"test\": test_size\n",
    "    }\n",
    "    json.dump(train_test_info, open(dest_path + 'datainfo.txt', 'w'))\n",
    "    write_tfrecord(train_label, dest_path + 'train_label')\n",
    "    write_tfrecord(train_unlabel, dest_path + 'train_unlabel')\n",
    "    write_tfrecord(test, dest_path + 'test')\n",
    "    write_tfrecord(val, dest_path + 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.488731Z",
     "start_time": "2023-05-09T01:19:15.479896Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_train_test(df):\n",
    "    print('Create train - test - val: ')\n",
    "    train, test = train_test_split(df, test_size=0.3, shuffle=True)\n",
    "    train, val = train_test_split(train, test_size=0.2, shuffle=True)\n",
    "    train_ul, train_l = train_test_split(train, test_size=0.1, shuffle=True)\n",
    "    train_ul = train_ul.reset_index().drop(['index'], axis=1)\n",
    "    train_l = train_l.reset_index().drop(['index'], axis=1)\n",
    "    test = test.reset_index().drop(['index'], axis=1)\n",
    "    val = val.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "    data_info = {\n",
    "        \"train_unlabel\": train_ul.shape[0],\n",
    "        \"train_label\": train_l.shape[0],\n",
    "        \"validation\": val.shape[0],\n",
    "        \"test\": test.shape[0]\n",
    "    }\n",
    "\n",
    "    return data_info, train_ul, train_l, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T01:19:15.488800Z",
     "start_time": "2023-05-09T01:19:15.480168Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(indir, outdir, attacks):\n",
    "    data_info = {}\n",
    "    for attack in attacks:\n",
    "        print('Attack: {} ==============='.format(attack))\n",
    "        finput = '{}/{}_dataset.csv'.format(indir, attack)\n",
    "        df = preprocess(finput)\n",
    "        print(\"Writing...................\")\n",
    "        foutput_attack = '{}/{}'.format(outdir, attack)\n",
    "        foutput_normal = '{}/Normal_{}'.format(outdir, attack)\n",
    "        df_attack = df[df['label'] == 1]\n",
    "        df_normal = df[df['label'] == 0]\n",
    "        write_tfrecord(df_attack, foutput_attack)\n",
    "        write_tfrecord(df_normal, foutput_normal)\n",
    "\n",
    "        data_info[foutput_attack] = df_attack.shape[0]\n",
    "        data_info[foutput_normal] = df_normal.shape[0]\n",
    "\n",
    "    json.dump(data_info, open('{}/datainfo.txt'.format(outdir), 'w'))\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T04:54:29.167711Z",
     "start_time": "2023-05-09T04:35:05.367336Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "Attack: DoS ===============\n",
      "Reading from ../data/car-hacking/DoS_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  88954\n",
      "#Attack:  37451\n",
      "Writing...................\n",
      "Writing ../data/TFRecord//DoS================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37451it [00:40, 926.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../data/TFRecord//Normal_DoS================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88954it [01:34, 938.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: Fuzzy ===============\n",
      "Reading from ../data/car-hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  87888\n",
      "#Attack:  44486\n",
      "Writing...................\n",
      "Writing ../data/TFRecord//Fuzzy================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44486it [00:48, 913.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../data/TFRecord//Normal_Fuzzy================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87888it [01:37, 904.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: gear ===============\n",
      "Reading from ../data/car-hacking/gear_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  87928\n",
      "#Attack:  65283\n",
      "Writing...................\n",
      "Writing ../data/TFRecord//gear================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65283it [01:13, 892.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../data/TFRecord//Normal_gear================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87928it [01:35, 924.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: RPM ===============\n",
      "Reading from ../data/car-hacking/RPM_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  87997\n",
      "#Attack:  71372\n",
      "Writing...................\n",
      "Writing ../data/TFRecord//RPM================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71372it [01:18, 913.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../data/TFRecord//Normal_RPM================= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87997it [01:35, 924.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "main(\"../data/car-hacking\", \"../data/TFRecord/\", attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T05:04:02.298720Z",
     "start_time": "2023-05-09T05:04:02.291565Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../data/TFRecord//DoS': 37451,\n",
       " '../data/TFRecord//Normal_DoS': 88954,\n",
       " '../data/TFRecord//Fuzzy': 44486,\n",
       " '../data/TFRecord//Normal_Fuzzy': 87888,\n",
       " '../data/TFRecord//gear': 65283,\n",
       " '../data/TFRecord//Normal_gear': 87928,\n",
       " '../data/TFRecord//RPM': 71372,\n",
       " '../data/TFRecord//Normal_RPM': 87997}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info = json.load(open('../data/TFRecord/datainfo.txt'))\n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:40:30.800407Z",
     "start_time": "2023-05-09T07:40:30.798761Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_tfrecord_split(data, filename):\n",
    "    print('Writing {}================= '.format(filename))\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    for batch_data in iter(data):\n",
    "        for x, y in zip(batch_data['input_features'], batch_data['label']):\n",
    "            tfrecord_writer.write(serialize_example(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T07:40:46.947400Z",
     "start_time": "2023-05-09T07:40:31.479484Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 -92509 -370040\n",
      "Writing ../data/DoS/train_label================= \n",
      "Writing ../data/DoS/train_unlabel================= \n",
      "Writing ../data/DoS/test================= \n",
      "Writing ../data/DoS/val================= \n"
     ]
    }
   ],
   "source": [
    "# dataset = data_from_tfrecord('./Data/TFRecord/DoS', 1000, 0)\n",
    "DATASET_SIZE = data_info['../data/TFRecord//DoS']\n",
    "train_size = 500 * 1000\n",
    "train_label_size = 100 * 1000\n",
    "val_size = int((DATASET_SIZE - train_size) * 0.2)\n",
    "test_size = DATASET_SIZE - train_size - val_size\n",
    "print(train_size, val_size, test_size)\n",
    "dataset = tf.data.TFRecordDataset('../data/TFRecord/DoS')\n",
    "dataset = dataset.map(read_tfrecord)\n",
    "dataset = dataset.shuffle(2)\n",
    "train = dataset.take(train_size)\n",
    "train_label = train.take(train_label_size)\n",
    "train_unlabel = train.skip(train_label_size)\n",
    "val = dataset.skip(train_size)\n",
    "test = val.skip(val_size)\n",
    "val = val.take(val_size)\n",
    "batch_size = 10000\n",
    "train = train.batch(batch_size)\n",
    "test = test.batch(batch_size)\n",
    "val = val.batch(batch_size)\n",
    "\n",
    "write_tfrecord_split(train_label, '../data/DoS/train_label')\n",
    "write_tfrecord_split(train_unlabel, '../data/DoS/train_unlabel')\n",
    "write_tfrecord_split(test, '../data/DoS/test')\n",
    "write_tfrecord_split(val, '../data/DoS/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T05:40:14.937063Z",
     "start_time": "2023-05-09T05:38:43.429027Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 17.2 µs\n",
      "../data/car-hacking/Fuzzy_dataset.csv---------------------------\n",
      "Reading from ../data/car-hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  87888\n",
      "#Attack:  44486\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_test_split() got an unexpected keyword argument 'test_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m preprocess(file_name)\n\u001b[0;32m----> 7\u001b[0m data_info, train_ul, train_l, val, test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(attack)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath: \u001b[39m\u001b[38;5;124m'\u001b[39m, save_path)\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mcreate_train_test\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_train_test\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate train - test - val: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     train, val \u001b[38;5;241m=\u001b[39m train_test_split(train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     train_ul, train_l \u001b[38;5;241m=\u001b[39m train_test_split(train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_test_split() got an unexpected keyword argument 'test_size'"
     ]
    }
   ],
   "source": [
    "# Create training test\n",
    "%time\n",
    "for attack in attack_types[1:]:\n",
    "    file_name = '{}{}_dataset.csv'.format(dataset_path, attack)\n",
    "    print(file_name + '---------------------------')\n",
    "    df = preprocess(file_name)\n",
    "    data_info, train_ul, train_l, val, test = create_train_test(df)\n",
    "    save_path = '../data/{}/'.format(attack)\n",
    "    print('Path: ', save_path)\n",
    "    print('Writing train_unlabel.......................')\n",
    "    write_tfrecord(train_ul, save_path + \"train_unlabel\")\n",
    "    print('Writing train_label.......................')\n",
    "    write_tfrecord(train_l, save_path + \"train_label\")\n",
    "    print('Writing test.......................')\n",
    "    write_tfrecord(test, save_path + \"test\")\n",
    "    print('Writing val.......................')\n",
    "    write_tfrecord(val, save_path + \"val\")\n",
    "    print('Writing data info')\n",
    "    json.dump(data_info, open(save_path + 'datainfo.txt', 'w'))\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T05:55:13.739279Z",
     "start_time": "2023-05-09T05:55:13.687929Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: DoS ==============\n",
      "500000 -92509 -370040\n",
      "Writing ../data/DoS/train_label================= \n",
      "Debug=================\n",
      "<BatchDataset element_spec={'input_features': TensorSpec(shape=(None, 841), dtype=tf.int64, name=None), 'label': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)}>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(source_path, dest_path, DATASET_SIZE, train_size, train_label_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m train_test_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_unlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_size \u001b[38;5;241m-\u001b[39m train_label_size,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_label_size,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_size,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_size\n\u001b[1;32m     30\u001b[0m }\n\u001b[1;32m     31\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(train_test_info, \u001b[38;5;28mopen\u001b[39m(dest_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatainfo.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m \u001b[43mwrite_tfrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m write_tfrecord(train_unlabel, dest_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_unlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m write_tfrecord(test, dest_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m, in \u001b[0;36mwrite_tfrecord\u001b[0;34m(data, filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m      5\u001b[0m tfrecord_writer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mTFRecordWriter(filename)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m()):\n\u001b[1;32m      7\u001b[0m     tfrecord_writer\u001b[38;5;241m.\u001b[39mwrite(serialize_example(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      8\u001b[0m tfrecord_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_info = json.load(open('../data/TFRecord/datainfo.txt'))\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "for attack in attack_types:\n",
    "    print(\"Attack: {} ==============\".format(attack))\n",
    "    source = '../data/TFRecord//{}'.format(attack)\n",
    "    dest = '../data/{}/'.format(attack)\n",
    "    train_test_split(source, dest, data_info[source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_test_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_unlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtrain_size\u001b[49m \u001b[38;5;241m-\u001b[39m train_label_size,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_label_size,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_size,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_size\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(data_info, \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatainfo.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_size' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_info = {\n",
    "        \"train_unlabel\": train_size - train_label_size,\n",
    "        \"train_label\": train_label_size,\n",
    "        \"validation\": val_size,\n",
    "        \"test\": test_size\n",
    "}\n",
    "json.dump(data_info, open(save_path + 'datainfo.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/TFRecord/datainfo.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/TFRecord/datainfo.txt'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normal_size = 0\n",
    "data_info = json.load(open('./Data/TFRecord/datainfo.txt'))\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "for attack in attack_types:\n",
    "    normal_size += data_info['./Data/TFRecord/Normal_{}'.format(attack)]\n",
    "sources = ['./Data/TFRecord/Normal_{}'.format(a) for a in attack_types]\n",
    "dest = './Data/Normal/'\n",
    "train_test_split(sources, dest, normal_size, train_size=500*1000*4*3, train_label_size=100*1000*4*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_test_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_unlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtrain_size\u001b[49m \u001b[38;5;241m-\u001b[39m train_label_size,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_label_size,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_size,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_size\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(data_info, \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatainfo.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_size' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_info = {\n",
    "        \"train_unlabel\": train_size - train_label_size,\n",
    "        \"train_label\": train_label_size,\n",
    "        \"validation\": val_size,\n",
    "        \"test\": test_size\n",
    "}\n",
    "json.dump(data_info, open(save_path + 'datainfo.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for attack in attack_types[1:]:\n",
    "    print('Attack: {} ==============='.format(attack))\n",
    "    file_name = '{}{}_dataset.csv'.format(dataset_path, attack)\n",
    "    df = preprocess(file_name)\n",
    "    write_tfrecord(df[df['label'] == 1], './Data/TFRecord/{}'.format(attack))\n",
    "    write_tfrecord(df[df['label'] == 0], './Data/TFRecord/Normal_{}'.format(attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
