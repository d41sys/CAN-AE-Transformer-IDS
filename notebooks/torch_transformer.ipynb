{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:37.705491Z",
     "start_time": "2023-06-22T07:51:37.498780Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:01.586664Z",
     "start_time": "2023-06-22T07:51:01.582961Z"
    }
   },
   "outputs": [],
   "source": [
    "def hex_to_int(hex_value):\n",
    "    return int(hex_value, base=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:01.590351Z",
     "start_time": "2023-06-22T07:51:01.585745Z"
    }
   },
   "outputs": [],
   "source": [
    "def hex_string_to_array(hex_string):\n",
    "    if hex_string == 'z':\n",
    "        return []\n",
    "    else:\n",
    "        return list(map(hex_to_int, hex_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:01.590592Z",
     "start_time": "2023-06-22T07:51:01.589063Z"
    }
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, d_in, d_out):  # config.slsum_count, config.dnn_out_d\n",
    "        super(DNN, self).__init__()\n",
    "        self.l1 = nn.Linear(d_in, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('x: ', x.numpy()[0])\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = F.relu(self.l3(out))\n",
    "        # print('dnn out: ', out.detach().numpy()[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:01.603972Z",
     "start_time": "2023-06-22T07:51:01.600274Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Transformer'\n",
    "        self.slide_window = 2\n",
    "        self.slsum_count = int(math.pow(16, self.slide_window))  # 滑动窗口计数的特征的长度 n-gram?\n",
    "        self.dnn_out_d = 8  # 经过DNN后的滑动窗口计数特征的维度 Dimensions of sliding window count features after DNN\n",
    "        self.head_dnn_out_d = 32\n",
    "        self.d_model = self.dnn_out_d + self.head_dnn_out_d  # transformer的输入的特征的维度, dnn_out_d + 包头长度 The dimension of the input feature of the transformer, dnn_out_d + header length\n",
    "        self.pad_size = 100\n",
    "        self.max_time_position = 10000\n",
    "        self.nhead = 5\n",
    "        self.num_layers = 3\n",
    "        self.gran = 1e-6\n",
    "        self.log_e = 2\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        self.classes_num = 3\n",
    "        self.batch_size = 10\n",
    "        self.epoch_num = 5\n",
    "        self.lr = 0.001\n",
    "        self.train_pro = 0.8  # 训练集比例 Ratio of training set\n",
    "\n",
    "        self.data_root_dir = '../data/car-hacking'\n",
    "        self.sl_sum_dir = '../data/car_hacking-data_slide_count_' + str(\n",
    "            self.slide_window) + '_arr'\n",
    "        self.time_dir = '../data/car_hacking-data_time'\n",
    "        self.names_file = '../data/name_class_CICIDS_3.csv'\n",
    "        self.model_save_path = '../model/' + self.model_name + '/'\n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.mkdir(self.model_save_path)\n",
    "        self.result_file = '/Users/d41sy/Desktop/sch/coding/ml-ids/result/trans8_performance.txt'\n",
    "\n",
    "        self.isload_model = False  # 是否加载模型继续训练 Whether to load the model and continue training\n",
    "        self.start_epoch = 24  # 加载的模型的epoch The epoch of the loaded model\n",
    "        self.model_path = 'model/' + self.model_name + '/' + self.model_name + '_model_' + str(self.start_epoch) + '.pth'  # 要使用的模型的路径 path to the model to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, max_time_position, device):\n",
    "        super(Time_Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, time_position):\n",
    "        out = x.permute(1, 0, 2)\n",
    "        out = out + nn.Parameter(time_position, requires_grad=False).to(self.device)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T06:10:48.454218Z",
     "start_time": "2023-06-22T06:10:48.448676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/car-hacking/Fuzzy_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "class MyTrans(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MyTrans, self).__init__()\n",
    "        self.dnn = DNN(config.slsum_count, config.dnn_out_d)\n",
    "        self.head_dnn = DNN(60, config.head_dnn_out_d)\n",
    "        self.position_embedding = Time_Positional_Encoding(config.d_model, config.max_time_position, config.device).to(\n",
    "            config.device)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=config.d_model, nhead=config.nhead).to(config.device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=config.num_layers).to(\n",
    "            config.device)\n",
    "        self.fc = nn.Linear(config.d_model, config.classes_num).to(config.device)\n",
    "        self.pad_size = config.pad_size\n",
    "        self.dnn_out_d = config.dnn_out_d\n",
    "        self.head_dnn_out_d = config.head_dnn_out_d\n",
    "\n",
    "    def forward(self, header, sl_sum, mask, time_position):\n",
    "        dnn_out = torch.empty((sl_sum.shape[0], self.dnn_out_d, 0))\n",
    "\n",
    "        for i in range(self.pad_size):\n",
    "            tmp = self.dnn(sl_sum[:, i, :]).unsqueeze(2)\n",
    "            dnn_out = torch.concat((dnn_out, tmp), dim=2)\n",
    "        dnn_out = dnn_out.permute(0, 2, 1)\n",
    "\n",
    "        head_dnn_out = torch.empty((header.shape[0], self.head_dnn_out_d, 0))\n",
    "        for i in range(self.pad_size):\n",
    "            tmp = self.head_dnn(header[:, i, :]).unsqueeze(2)\n",
    "            head_dnn_out = torch.concat((head_dnn_out, tmp), dim=2)\n",
    "        head_dnn_out = head_dnn_out.permute(0, 2, 1)\n",
    "\n",
    "        x = torch.concat((head_dnn_out, dnn_out), dim=2).permute(1, 0, 2)\n",
    "\n",
    "        out = self.position_embedding(x, time_position)\n",
    "        out = self.transformer_encoder(out, src_key_padding_mask=mask)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = torch.sum(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:01.594195Z",
     "start_time": "2023-06-22T07:51:01.592446Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_confusion(label_y, pre_y, path):\n",
    "    confusion = confusion_matrix(label_y, pre_y)\n",
    "    print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T07:51:01.599448Z",
     "start_time": "2023-06-22T07:51:01.597347Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_result(fin, label_y, pre_y, classes_num):\n",
    "    if classes_num > 2:\n",
    "        accuracy = accuracy_score(label_y, pre_y)\n",
    "        macro_precision = precision_score(label_y, pre_y, average='macro')\n",
    "        macro_recall = recall_score(label_y, pre_y, average='macro')\n",
    "        macro_f1 = f1_score(label_y, pre_y, average='macro')\n",
    "        micro_precision = precision_score(label_y, pre_y, average='micro')\n",
    "        micro_recall = recall_score(label_y, pre_y, average='micro')\n",
    "        micro_f1 = f1_score(label_y, pre_y, average='micro')\n",
    "        print('  -- test result: ')\n",
    "        fin.write('  -- test result: \\n')\n",
    "        print('    -- accuracy: ', accuracy)\n",
    "        fin.write('    -- accuracy: ' + str(accuracy) + '\\n')\n",
    "        print('    -- macro precision: ', macro_precision)\n",
    "        fin.write('    -- macro precision: ' + str(macro_precision) + '\\n')\n",
    "        print('    -- macro recall: ', macro_recall)\n",
    "        fin.write('    -- macro recall: ' + str(macro_recall) + '\\n')\n",
    "        print('    -- macro f1 score: ', macro_f1)\n",
    "        fin.write('    -- macro f1 score: ' + str(macro_f1) + '\\n')\n",
    "        print('    -- micro precision: ', micro_precision)\n",
    "        fin.write('    -- micro precision: ' + str(micro_precision) + '\\n')\n",
    "        print('    -- micro recall: ', micro_recall)\n",
    "        fin.write('    -- micro recall: ' + str(micro_recall) + '\\n')\n",
    "        print('    -- micro f1 score: ', micro_f1)\n",
    "        fin.write('    -- micro f1 score: ' + str(micro_f1) + '\\n\\n')\n",
    "        report = classification_report(label_y, pre_y)\n",
    "        fin.write(report)\n",
    "        fin.write('\\n\\n')\n",
    "    else:\n",
    "        accuracy = accuracy_score(label_y, pre_y)\n",
    "        precision = precision_score(label_y, pre_y)\n",
    "        recall = recall_score(label_y, pre_y)\n",
    "        f1 = f1_score(label_y, pre_y)\n",
    "        print('  -- test result: ')\n",
    "        print('    -- accuracy: ', accuracy)\n",
    "        fin.write('    -- accuracy: ' + str(accuracy) + '\\n')\n",
    "        print('    -- recall: ', recall)\n",
    "        fin.write('    -- recall: ' + str(recall) + '\\n')\n",
    "        print('    -- precision: ', precision)\n",
    "        fin.write('    -- precision: ' + str(precision) + '\\n')\n",
    "        print('    -- f1 score: ', f1)\n",
    "        fin.write('    -- f1 score: ' + str(f1) + '\\n\\n')\n",
    "        report = classification_report(label_y, pre_y)\n",
    "        fin.write(report)\n",
    "        fin.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T05:21:09.684290Z",
     "start_time": "2023-06-14T05:21:09.679343Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T05:21:10.428617Z",
     "start_time": "2023-06-14T05:21:10.422136Z"
    }
   },
   "outputs": [],
   "source": [
    "fin = open(config.result_file, 'a')\n",
    "fin.write('-------------------------------------\\n')\n",
    "fin.write(config.model_name + '\\n')\n",
    "fin.write('begin time: ' + str(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())) + '\\n')\n",
    "fin.write('data root dir: ' + config.data_root_dir + '\\n')\n",
    "fin.write('sl_sum_dir: ' + config.sl_sum_dir + '\\n')\n",
    "fin.write('names_file: ' + config.names_file + '\\n')\n",
    "fin.write('d_model: ' + str(config.d_model) + '\\t pad_size: ' + str(config.pad_size) + '\\t nhead: ' + str(config.nhead)\n",
    "          + '\\t num_layers: ' + str(config.num_layers) + '\\t head_dnn_out_d: '+ str(config.head_dnn_out_d) +'\\n')\n",
    "fin.write(\n",
    "    'batch_size: ' + str(config.batch_size) + '\\t train pro: ' + str(config.train_pro) + '\\t learning rate: ' + str(\n",
    "        config.lr) + '\\n\\n')\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T05:21:11.465357Z",
     "start_time": "2023-06-14T05:21:11.454403Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetSLForTransDNNT(Dataset):\n",
    "    def __init__(self, root_dir, sl_sum_dir, time_dir, names_file, pad_size, embed, max_time_position, gran, log_e,\n",
    "                 transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.sl_sum_dir = sl_sum_dir\n",
    "        self.time_dir = time_dir\n",
    "        self.names_file = names_file\n",
    "        self.transform = transform\n",
    "        self.size = 0\n",
    "        self.name_list = []\n",
    "        self.pad_size = pad_size\n",
    "        self.embed = embed\n",
    "        self.max_time_position = max_time_position\n",
    "        self.gran = gran\n",
    "        self.log_e = log_e\n",
    "\n",
    "        if not os.path.isfile(self.names_file):\n",
    "            print(self.names_file + 'does not exist!')\n",
    "        f = open(self.names_file, 'r')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            self.name_list.append(line)\n",
    "            self.size += 1\n",
    "\n",
    "        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / self.embed)) for i in range(self.embed)] for pos in range(self.max_time_position)])\n",
    "        print(self.pe)\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])  # 偶数列用sin Use sin for even columns\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])  # 奇数列用cos Use cos for odd columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_time(self, time_position):\n",
    "        # 根据时间位置切分出对应的位置编码\n",
    "        # Segment the corresponding position code according to the time position\n",
    "        pe = torch.index_select(self.pe, 0, time_position)\n",
    "        return pe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.name_list[idx]\n",
    "        feature_csv_path = os.path.join(self.root_dir, item[0])\n",
    "        label = eval(item[1])\n",
    "        if not os.path.exists(feature_csv_path):\n",
    "            print(feature_csv_path, ' does not exist!')\n",
    "            return None\n",
    "        feature_f = open(feature_csv_path, 'r')\n",
    "        feature_reader = csv.reader(feature_f)\n",
    "\n",
    "        print(feature_f)\n",
    "        print(feature_reader)\n",
    "\n",
    "        ip_header = feature_reader.__next__()[1:]\n",
    "        ip_header = np.array(list(map(hex_string_to_array, list(ip_header))))[:self.pad_size]\n",
    "        tcp_header = feature_reader.__next__()[1:]\n",
    "        tcp_header = np.array(list(map(hex_string_to_array, list(tcp_header))))[:self.pad_size]\n",
    "        header = np.hstack((ip_header, tcp_header))\n",
    "        header = torch.from_numpy(header)\n",
    "\n",
    "        print(header)\n",
    "\n",
    "        slsum_csv_path = os.path.join(self.sl_sum_dir, item[0])\n",
    "        if not os.path.exists(slsum_csv_path):\n",
    "            print(slsum_csv_path, 'does not exist!')\n",
    "            return None\n",
    "        sl_sum = pd.read_csv(slsum_csv_path, header=None, index_col=None).values[:self.pad_size]\n",
    "        sl_sum = torch.from_numpy(np.array(sl_sum))\n",
    "\n",
    "        ori_seq_len = header.shape[0]\n",
    "        pad_len = self.pad_size - ori_seq_len\n",
    "\n",
    "        header = F.pad(header.T, (0, pad_len)).T.numpy()\n",
    "        sl_sum = F.pad(sl_sum.T, (0, pad_len)).T.numpy()\n",
    "\n",
    "        if pad_len == 0:\n",
    "            mask = np.array([False] * ori_seq_len)\n",
    "        else:\n",
    "            mask = np.concatenate((np.array([False] * ori_seq_len), np.array([True] * pad_len)))  # padding mask\n",
    "\n",
    "        # time\n",
    "        time_csv_path = os.path.join(self.time_dir, item[0])\n",
    "        time_record = pd.read_csv(time_csv_path, header=None, index_col=None).values[0][:self.pad_size]\n",
    "        len_time_record = len(time_record)\n",
    "        for i in range(len_time_record):\n",
    "            value = round(math.log(round(time_record[i] / self.gran) + 1, self.log_e))\n",
    "            time_record[i] = value\n",
    "        for j in range(self.pad_size - len_time_record):\n",
    "            time_record = np.append(time_record, time_record[len_time_record - 1])\n",
    "\n",
    "        time_feature = self.get_time(torch.IntTensor(time_record))\n",
    "\n",
    "        sample = {'header': header, 'sl_sum': sl_sum, 'mask': mask, 'time': time_feature, 'label': label, 'idx': idx}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDatasetSLForTransDNNT(config.data_root_dir, config.sl_sum_dir, config.time_dir, config.names_file, config.pad_size, config.d_model, config.max_time_position, config.gran, config.log_e)\n",
    "size = len(dataset)\n",
    "print(dataset.name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(config\u001b[39m.\u001b[39mtrain_pro \u001b[39m*\u001b[39m size)\n\u001b[1;32m      2\u001b[0m test_size \u001b[39m=\u001b[39m size \u001b[39m-\u001b[39m train_size\n\u001b[1;32m      3\u001b[0m train_dataset, test_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(dataset, [train_size, test_size])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "train_size = int(config.train_pro * size)\n",
    "test_size = size - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=config.batch_size)\n",
    "print('finish load data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
